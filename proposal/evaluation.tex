\section{Evaluation methods}

We are going to do separate evaluations of the two proposed
visualization tools. we will build for tow different tasks of natural
language processing.

\begin{itemize}

\item Clustering visualization. In order to evaluate the clustering
visualization tool, we plan to compare the labeling process using our
tool to that of a traditional labeling
process of doing word clustering, where the participants are required
to manually label each word according to different categories on a
Microsoft Excel or similar chart software.

\textit{Participants}: We plan to gather 10 participants from UW
undergraduate/graduate students.  \textit{Experiment}: The
participants are divided into two groups, Each group will complete two
standard linguistic clustering tasks with similar work load and
difficulty. One of the group will conduct the task with help of our
visualization tool first, the other will do the task without
visualization first. Then the tow groups switch task.

Evaluation: Both time consumption and accuracy of the two groups of
participants will be evaluated.

\item Tree Parsing. Because of the difficulty of the Tree Parsing
tasks, there is actually no reliable way for people to conduct such
tree parsing without tedious training. So our evaluation aim to
discover how this good this tool can actually turn something almost
impossible to reality.

Participants: We will gather 10 native speaker participants form
undergraduate/graduate CSE students.  Experiment: participants will
complete two standard linguistic parcing tasks with similar work load
and difficulty. One of the group will conduct the task with help of
our visualization tool first, the other will do the task without
visualization first. Then the two groups switch task.

\end{itemize}

Evaluation: Both time consumption and accuracy of the two groups of
participants will be evaluated.

