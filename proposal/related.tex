\section{Related work}
Manual NLP annotation is well-known for its tedium and large amount of data. To generate a annotated training set requires much human effort. Annotators are also prone to make mistakes during the tedious and long annotating process. Researchers are trying to address these problems by two means: 1) develop visualization tools to improve annotation efficiency as well as reduce the error rate in annotation; 2) adopt crowdsourcing to enable collaborative annotation that accelerates the process of annotation.

Most related in scope is \cite{yan2012collaborative} which provides a collaborative tool to assist annotators in tagging of complex Chinese and multilingual linguistic data. It visualizes a tree model that represents the complex relations across different linguistic elements to reduce the learning curve. Besides it proposes a web-based collaborative annotation approach to meet the large amount of data. Their tool only focuses on a specific area that is complex multilingual linguistic data, whereas our work is trying to address how to generate a visualization model for general data sets.

There are other exisiting annotation tools that provide a graphic user interface to help annotators visualize the annotation process. MMAX2 \cite{mueller06b}, Gate \cite{gate} both provide a customizable methodology for annotating document at different granularity. But they fail to provide an intuitive user interface, which makes unexperienced annotator not easy to get started. Besides they do not allow collaborative annotation. \cite{brat} is an web-based tool for text annotation. It has a neat and clear visualization design. But it uses specific tags which requires the annotators have background knowledge in NLP. However, we assume that the annotators are normal users who don't possess any background in NLP, and aim to provide an intuitive interface that everyone know how to use without training.

Crowdsourcing now is recognized as a growing and promising approach in NLP. Many related works focus on conceptual study and formalization of crowdsourcing. For instance, \cite{quinn2009taxonomy} categorizes crowdsourcing into seven genres: Mechanized Labor, Game with a Purpose (GWAP), Widom of Crowds, Crowdsourcing, Dual-Purpose Work, Grand Serarch, Human-based Genetic Algorithms and Knowledge Collection from Volunteer Contributors. Other works, such as \cite{abekawa2010community} and \cite{irvine2010using}, develops a specific tool and verifies the feasibility and benefit of crowdsourcing. Nevertheless, we seek to provide an intuitive visualization to lower the barrier to get started on crowdsourcing. 
