\section{Introduction}

With the popularity of the Intenet, there are huge amount of text in the web, and their size is still growing quickly. In order to use them, it is of utmost importance to develop automatic nature language processing (NLP) systems to handle the big data. 

%A typical NLP system would first employ syntactic processing and then employ semantic processing.  Syntactic processing is often task independent. It aims to convert the raw text into some machine friendly structures. The pipeline often includes tokenization, POS tagging, parsing, name entity recognition and coreference. Semantic processing is often task dependent. It tries to exploit useful information from the text for an end  task. 

It has been wildly accepted that statistical machine learning approaches are very effective for most NLP problems, such as parsing, relation extractions, question answering and so on. However, there is a significant limitation of all statistical approaches, they need a lot of training data to train the model. These training data are labeled by annotators. In most cases, annotators manually predict the desired outputs for the inputs. The algorithms then learn the statistics and models from the training data and use them to automatically predict the output of the unlabeled data.   

Generally, labeling datasets for machine learning problems (often classification) is time consuming and tedious. To make matters worse, it is even harder for annotators to label NLP problems than to label standard classification problems. For a standard classification problem, human annotators are given a set of labels and a list of objects. Their jobs are to choose a label for each object. Since objects are usually independent of each other, the prediction is local and straightforward. Besides, labeling errors, if any, would not affect the rest of the dataset. Therefore, the user interface for the annotations could be very naive: plain text or Excel tables are often enough in many cases. 


But for NLP annotations, the outputs are often structured predictions. That is, each prediction is depending on some other prediction. For example, Figure 1 shows how parsing algorithm converts a sentence into a tree. It is hard for Annotates to decide the position of a single word in the tree before drawing the whole tree. Besides, a large group of NLP problems are related to clustering, such as coreference, which clusters mentions in the article of the same entities. Unlike classification, annotates must understand the big picture in order to correctly label the clusters of the data. Transitivity makes the annotation very tricky. For example, after merging pairs of points, the annotator has created two clusters $\{A_1,\ldots,A_{10}\}$ and $\{B_1,\ldots,B_{10}\}$. He then accidentally merges $A_1$ and $B_2$. If this operation is incorrect, it would immediately cause $100$ pairs of errors, which is really a disaster. 


The challenges listed above makes the labeling process extremely uncomfortable for normal annotators. Firstly, annotators must spend a lot of time to understand the inner structure of the data before labeling anything. Secondly, annotators would often revisit and edit their labels. For example, during coreference annotation, at first annotators did not know there are two ``Obama"s in the article so he simply annotate all ``Obama" as ``Barack Obama". After a while, he have to go back to fix them because he noticed ``Mitchell Obama". False annotations tend to occur during such trial and errors. In fact, even trained linguistic must spend a lot of time to label NLP datasets. For example, it costs 8 years to create the Penn Tree Bank, a labeled set of parsed trees. 

Nowadays, there are many people work at crowdsourcing platforms like Mechanical Turks and Odesk. This phenomena provides an opportunity to quickly collect a large amount of training data. But most of them are normal people, having little knowledge about linguistic, and having no reason to be patient enough. It is impractical to ask them to label over plain text files or Excel tables, since they would switch to other easier profitable tasks.