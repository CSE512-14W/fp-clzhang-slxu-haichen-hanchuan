\section{Proposed System}

In this project, we aim to develop a visualized toolkit for crowdsourcing NLP annotations. The target audience are normal people with little knowledge and patience. The toolkit would allow them to quickly label NLP datasets.

There are two key properties of our toolkit: firstly, annotators could interact with the points to understand the data in an up-to-date way. For example, any partial annotations reflect annotators' partial understanding. So they would expect immediate feedback from the toolkit. Secondly, the toolkit should enable and even encourage trial and errors. It would not take any edits from the users as granted, but treat the edits as clues to better render the data to the annotator. When the annotator finish a labeling task, he should be satisfied with the global outcome and confident. In the above $\{A_1,\ldots,A_{10}\}$ and $\{B_1,\ldots,B_{10}\}$ example, it is likely that $A_1$ and $B_2$ are hard to distinguish when the pair is seen separately from the rest of the data. But if the toolkit could immediate show a big cluster $\{A_1,\ldots,A_{10}, B_1,\ldots,B_{10}\}$ after accidentally merge $A_1$ with $B_2$, the annotators would have a good chance to change their mind and fix the errors. 

 In this project, we would focus on two important kinds of NLP annotations: a tree prediction (\eg\  parsing) and a graph prediction (\eg\ coreference). But we would keep in mind that the toolkit should be easily extensible to any NLP problems. 

\subsection{Annotate Tree Prediction}
\subsection{Annotate Graph Prediction}