% 8 pages + 2 pages of refs

\documentclass[11pt]{article}

\usepackage{naaclhlt2013}
%\usepackage{acl2013}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{url}
\usepackage{subfigure}
\usepackage{array}
\usepackage{sparklines}
\usepackage{amsmath,amsfonts,amssymb,amsthm}



\usepackage{paralist}
\usepackage{color}
%\usepackage{setspace}



\renewcommand{\baselinestretch}{0.98}
%\let\env=\texttt
%\let\pkg=\textsf
%\definecolor{sparkrectanglecolor}{rgb}{0.8,0.95,0.8}
%\definecolor{sparkspikecolor}{rgb}{1,0,0}
%\setlength{\sparkdotwidth}{1.3pt}
%\setlength{\sparklinethickness}{.3pt}

\DeclareMathOperator*{\argmax}{arg\,max}
  %\setlength\titlebox{6.5cm}    % Expanding the titlebox

\input{weld-defns-edited}

\def\upcase{\expandafter\makeupcase}



\newcommand{\hrone}{temporal functionality heuristic}
\newcommand{\hrtwo}{temporal burstiness heuristic}
\newcommand{\hrthree}{one event-mention per discourse heuristic}
\newcommand{\hrfour}{Tense Consistency Heuristic}


\newcommand{\temporal}{Temporal Hypotheses}
\newcommand{\sys}{\mbox{\sc NewsSpike}}   % afar
\newcommand{\kylin}{\mbox{\sc Kylin}}
\newcommand{\tr}{\mbox{\sc TextRunner}}
\newcommand{\E}{\mbox{$\mathbf e$}}

%\newcommand{\mtt}[1]{\mbox{$\tt{#1}$}}
\newcommand{\bmtt}[1]{\mbox{$\tt{\mathbf{#1}}$}}

\newcommand{\eat}[1]{}

\newcommand{\Eec}{\mbox{\em extracted event candidate}}
%\newcommand{\Eec}{Extracted Event Candidate}
%\newcommand{\Eec}{\mbox{Extracted Event Candidate}}
\newcommand{\eec}{\mbox{EEC}}
\newcommand{\bag}{\mbox{EEC-set}}

\newcommand{\mtt}[1]{{\it #1}}


\title{Harvesting Parallel News Streams to Cluster Relation Phrases}

%\title{Distant Supervision for Information Extraction of Polymorphic Tuples}
%\title{Distant Supervision for Information Extraction
%                   without Relational Exlcusivity}


%\author{Congle Zhang, Daniel S. Weld \\
%  Computer Science \& Engineering \\
%  University of Washington\\
%  Seattle, WA 98195, USA \\
%  {\tt \{clzhang,weld\}@cs.washington.edu} \\}
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  {\tt email@domain} \\}

%\date{}

\begin{document}
\maketitle

\begin{abstract}
The distributional hypothesis, which states that words that occur in
similar contexts tend to have similar meanings, has inspired several
Web mining algorithms for clustering semantically equivalent phrases.
%(dirt, curran, resolver, usp, berant?).
Unfortunately, these methods have several drawbacks, such as confusing
synonyms with antonyms and causes with effects. This paper introduces
three Temporal Correspondence Heuristics, that characterize
regularities in parallel news streams, and shows how they may be used
for high precision relation clustering. We encode the heuristics in a
probabilistic graphical model to create the \sys\ algorithm for mining
news streams. We present experiments demonstrating that
\sys\ significantly outperforms several competitive baselines.  In
order to spur further research, we provide a large annotated corpus of
timestamped news articles as well as the clusters produced by \sys.
\end{abstract}

\input{intro}
\input{model}
\input{inferencelearning}
\input{exp}
\input{related}
\input{concl}
%\input{distant}

%\input{learn}
%\input{inference}




%The source code of our system, its output, and all data annotations are available at {\tt http://cs.uw.edu/homes/raphaelh/mr}.
%
%\section*{Acknowledgments}
%We thank Sebastian Riedel and Limin Yao for sharing their data and providing valuable advice.
%This material is based upon work supported by a WRF / TJ Cable Professorship, a gift from Google and
%by the Air Force Research Laboratory (AFRL) under prime contract no. FA8750-09-C-0181.  Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).

\bibliographystyle{naaclhlt2013}
\bibliography{general,acl11}

\end{document}


\section{Introduction}

Paraphrasing, the task of finding sets of semantically equivalent
surface forms, is crucial to many natural language processing
applications, including relation extraction~\cite{bhagat2008large},
question answering~\cite{faderparaphrase},
summarization~\cite{barzilay1999information} and machine
translation~\cite{callison2006improved}. While the benefits of
paraphrasing have been demonstrated, creating a large-scale corpus of
high precision paraphrases remains a challenge --- especially for
relation phrases.

Many researchers have considered generating paraphrases by mining the
Web guided by the {\em distributional hypothesis}, which states that
words occurring in similar contexts tend to have similar
meanings~\cite{harris1954distributional}. For example,
DIRT~\cite{lin2001discovery} and Resolver~\cite{yates2009unsupervised}
identify synonymous relation phrases by their distributions of the
arguments. However, the distributional hypothesis has several
drawbacks. First, it can confuse antonyms with synonyms because
antonymous phrases appear in similar contexts as often as synonymous
phrases. For the same reasons, it also often confuses causes with
effects. For example, {DIRT} reports that the closest phrase to
\mtt{fall} is \mtt{rise}, and the closest phrase to \mtt{shoot} is
\mtt{kill}.\footnote{\url{http://demo.patrickpantel.com/demos/lexsem/paraphrase.htm}}
Second, the distributional hypothesis relies on statistics over large
corpora to produce accurate similarity statistics.  It remains unclear
how to accurately cluster less frequent relations with the
distributional hypothesis.

%Resolver only targets relations appearing at least 25 times.


Another common approach employs the use of parallel corpora. News
articles are an interesting target, because there often exist articles
from different sources describing the same daily events. This peculiar
property allows the use of the temporal assumption, which assumes that
phrases in articles published at the same time tend to have similar
meanings. For example, the approaches by
Dolan~\etal~\shortcite{dolan2004unsupervised} and
Barzilay~\etal~\shortcite{barzilay2003learning} identify pairs of
sentential paraphrases in similar articles that have appeared in the
same period of time. While these approaches use temporal information
as a coarse filter in the data generation stage, they still largely
rely on text metrics in the prediction stage. This not only
reduces precision, but also limits the discovery of paraphrases with
dissimilar surface strings.

The goal of our research is to develop a technique for clustering
large numbers of relation phrases with high precision, using only
minimal human effort. The key to our approach is a joint cluster model
using the temporal attributes of news streams, which allows us to
identify semantic equivalences of relation phrases at greater
precision. In summary, this paper makes the following contributions:

\bi
\item We formulate a set of three {\em temporal correspondence heuristics}
that characterize regularities over parallel news streams.

\item We develop a novel program, \sys, based on a probabilistic
  graphical model that jointly encodes these heuristics. We present
  inference and   learning algorithms for our model.
\item We present a series of detailed experiments demonstrating that
  \sys\ outperforms several competitive baselines, and show through
  ablation tests how each of the temporal heuristics affects
  performance.
\item To spur further research on this topic, we provide both our
  generated paraphrase clusters and a corpus of 0.5M time-stamped news
  articles, collected over a period of 50 days from hundreds of news
  sources.
    %To our knowledge, there exists no such dataset for academic use.
\ei





%                               OpenIE
%News Stream -----------------> shallow relation triples
%                                                                 | group by entity pairs
%                                                                V
%                                                      entities and relation phrases
%                                                      A,B: {r1,r2,r3,r4,r5,r6}
%                                                      C,D:...
%                                                      E,F:...
%
%news spike
%features
%             ------->             <----------
%                             JOINT
%                          INFERENCE
%                           based on
%                     temporal hypotheses
%                                |
%                               V
%                                                      entities with relation phrases for
%                                                          predominant relations
%                                                      A,B: {r1, r2, r3}
%                                                      C,D: {r1, r4, r5}
%                                                              |
%                                                              |
%                                                              V
%                                                      semantically equivalent
%                                                      relation phrases
%
%
%
%
%
%
%
%For each entity pair and each given day, there exists at most one binary relation that is predominant in the news.
%
%
%
%Assumption: Each article describes an event and for each article, there exists a predominant binary relation that describes the event.
%
%
%goal: predominant events and their relation phrase representation
%
%
%
%
%
%
%%http://thesaurus.com/browse/dominant?s=t
%% prevalent, predominant, prevailing, principal
%% dominant, salient, governing
%
%
%
%
%%Open Information Extraction system can produce argument-relation-argument tuples, \ie {\tt (Barrack Obama, was born in, Hawaii)}. A well-known limitation of open IE system preventing it from wider applications is that the relation phrase it extracted are extremely diversified: relation phrases pointing to the same relation or event are bearing different surface strings. In this work, we are trying to cluster relation cluster phrases denoting the same relation. Such synonymous relation phrases have great potential for downstream applications, \eg information extraction, question answering, machine translation.
%
%Clustering relations phrases is a fundamental problem in natural language processing. A high quality relation clustering system, which finds synonymous relations\footnote{As described in~\cite{bhagat2013paraphrase}, synonymous mean that replacing the relation phrase by another phrase in the appropriate context results in the same meaning sentence.}, could have great potential in downstream applications. In the case of Open Information Extraction, which produces argument-relation-argument tuples, a well-known limitation is that many synonymous relation phrases bear different surface strings. They cannot be treated as the same relation until we correctly cluster them. Statistical NLP techniques often use relation phrases as features for machine learning algorithms. But extremely high variation of the relation phrases often results in challenging high dimension learning problems.
%
%%In case of Relation Extraction(RE), which classify arguments into pre-defined relations, relation phrases are often represented as the syntactic dependency pathes between the arguments. They can be used as critical features for machine learning algorithms~\cite{mintz09}, but the syntactic variations in these pathes yield extremely high dimensional problems, which is very hard. In case of the event co-reference system, good relation clusters may be the key reason for success.
%
%The {\em Distributional Hypothesis}, which states that words occurring in similar contexts tend to have similar meanings~\cite{bhagat2013paraphrase}, is widely used in clustering algorithms. Many similarity metrics have been developed based on it. For example, the {\sc DIRT} system~\cite{lin2001discovery} computes the mutual information statistics to identify the similarity between relations represented in dependency paths. {Resolver}~\cite{yates2009unsupervised} uses a similarity measure called Extracted Shared Property to cluster relations on web-scale extractions. However, the distributional hypothesis has several weaknesses. It can confuse antonyms with synonyms, and causes with effects, because antonymous phrases appear in similar contexts equally as often as synonymous phrases. For example, {\sc DIRT} reports that the closest phrase to \mtt{fall} is \mtt{rise}, and the closest phrase to \mtt{shoot} is \mtt{kill} \footnote{\url{http://demo.patrickpantel.com/demos/lexsem/paraphrase.htm}}. Secondly, the distributional hypothesis relies on statistics over large corpora to produce accurate similarity statistics. It is unclear how to accurately cluster less frequent relations with the distributional hypothesis.
%
%%But long tail is a prominent phenomena that many relation phrases appear very few times even in web-scale dataset. It is very challenging for methods based on distributional hypothesis to design effective similarity metrics for these relation phrases. Hence, many works only report accuracy on their best predictions. The performance could drop sharply if users want more relation clusters than top ones.
%
%%Thirdly, it is unclear how these methods can incrementally produce new accurate clusters, because small amount of new inputs will not change the distribution largely, and the top clusters will keep the same.
%
%%Research also propose probabilistic models \eg LDA-based models~\cite{yao2011structured}, MLN-based models~\cite{poon2009unsupervised} to remove the syntactic variations of the relation phrases, and to get their semantics. These methods do not directly compute distribution similarity, but still depend on relation phrase and document co-occurrence to provide evidences. Thereby they also often suffer from the limitations of the distributional hypothesis.
%
%
%\begin{figure}
%\centering
%\includegraphics[width=3.1in]{systemoverview.pdf}
%\caption{System overview: exploiting temporal correspondence hypothesis to cluster relations in the news stream.
%%: given news stream, an IE system generates event candidates and relation candidates that probably describe the event; features and constraints are generated by temporal correspondence hypothesis and then used in a supervised model to build relation clusters.
%}
%\label{systemoverview}
%\end{figure}
%
%In this work, we exploit the temporal properties of news articles to produce high-precision relation clusters. The starting point is that news articles published at the same time tend to describe the same real-world events. So long as we can recognize a set of relation phrases describing a particular event, these relations should compose a high quality cluster. Figure~\ref{systemoverview} illustrates the overview of this work. We first apply an information extraction system to extract event candidates and also relations most likely describing the events. Then we propose three \temporal\ and based on them, a set of {\em news spike} features and constraints are exploited. Finally, a sequence of supervised learning models are investigated to capture these features and constraints to generate relation clusters. This work makes the following contributions:
%
%%Parallel corpus, such as different versions of translation or news articles, has been used to produce paraphrases.
%\bi
%    \item We propose three Temporal Correspondence Hypotheses that are useful for high-precision relation clustering.
%    \item We introduce a sequence of methods to exploit these Temporal Correspondence Hypotheses.
%    \item Experiments demonstrate that our system outperforms many competitive baselines, including the state of the art relation clustering techniques and sentence-level paraphrase detection techniques.
%    \item We will release a large scale timestamped news corpus containing news from hundreds of resources. To our best knowledge, there has been no such dataset for academic usage yet.
%\ei

%\begin{figure*}[htb]
%\centering
%\subfigure[]{
%  \includegraphics[width=1.7in]{plate-model.pdf}
%  \label{fig-plate}
%}
%\subfigure[]{
%  \includegraphics[width=4.6in]{fig.pdf}
%  \label{fig-example}
%}
%    \vspace{-5pt}
%\caption{(a) Network structure depicted as plate model and (b) an
%  example network instantiation for the pair of entities {\tt Steve Jobs}, {\tt Apple}.}
%  \vspace{-5pt}
%\end{figure*}

\section{System Overview}
\label{section_framework}
\begin{figure}
\centering
\includegraphics[width=3.1in]{systemoverview.pdf}
\caption{\sys\ first applies open information extraction to articles in the news streams, obtaining shallow extractions with time-stamps. Next, an \Eec\ (\eec) is obtained after grouping daily extractions by
  argument pairs. Temporal features and constraints are developed
  based on our temporal correspondence heuristics and encoded into a
  joint inference model. The model finally creates the relation
  clusters by predicting the relation phrases that describe the \eec.
%System overview: exploiting the temporal hypotheses to cluster relations in the news streams
}
\label{systemoverview}
\end{figure}
The main goal of this work is to generate high precision relation
clusters.  News streams are a promising resource, since articles from
different sources tend to use semantically equivalent phrases to
describe the same daily events. For example, when a recent scandal
hit, headlines read: \mtt{``Armstrong steps down from Livestrong''};
\mtt{``Armstrong resigns from Livestrong''} and \mtt{``Armstrong cuts
  ties with Livestrong''}. From these we can conclude that the
following relation phrases are semantically similar: \mtt{\{step down
  from, resign from, cut ties with\}}.

To realize this intuition, our first challenge is to represent an
event. In practice, a question like {\it ``What happened to Armstrong
  and Livestrong on Oct 17?''} could often lead to a unique
answer. It implies that using an argument pair and a time-stamp could
be an effective way to identify an event (\eg\ \mtt{(Armstrong,
  Livestrong, Oct 17)} for the previous question). Based on this
observation, this paper introduces a novel mechanism to cluster
relations as summarized in Figure~\ref{systemoverview}.

%It has been noticed that entity pairs carry strong information to suggest relations and events~\cite{hasegawa2004discovering,sekine2005automatic}.


\sys\ first applies the ReVerb open information extraction (IE)
system~\cite{fader11} on the news streams to obtain a set of
$(a_1,r,a_2,t)$ tuples, where the $a_i$ are the arguments, $r$ is a
relation phrase, and $t$ is the time-stamp of the corresponding news
article. When $(a_1,a_2,t)$ suggests a real word event, the relation
$r$ of $(a_1,r,a_2,t)$ is likely to describe that event
(\eg\ \mtt{(Armstrong, resign from, Livestrong, Oct 17}). We call
every $(a_1, a_2, t)$ an {\em \Eec} (\eec), and every relation describing the event an {\em event-mention}.

For each \eec\ $(a_1,a_2,t)$, suppose there are $m$ extraction tuples
$(a_1,r_1,a_2,t)\ldots (a_1,r_m,a_2,t)$ sharing the values of $a_1,
a_2,$ and $t$. We refer to this set of extraction tuples as the {\em
  \bag}, and denote it $(a_1,a_2,t,\{r_1\ldots r_m\})$. All the
event-mentions in the \bag\ may be semantically equivalent and are
hence candidates for a good relation cluster.

Thus, the relation clustering problem becomes a prediction problem:
for each relation $r_i$ in the \bag, does it or does it not described
the hypothesized event?  We solve this problem in two steps. The next
section proposes a set of temporal correspondence heuristics that
partially characterize semantically equivalent \bag s. Then, in
Section~\ref{section_model}, we present a joint inference model
designed to use these heuristics to solve the prediction problem and
output relation clusters.




%In sum, we frame our relation clustering problem as finding these semantically equivalent relations in the \bag, and then generate the relation cluster.

\section{Temporal Correspondence Heuristics}
\label{section_temporal}
\theoremstyle{plain} \newtheorem{hypothesis}{H}



\newtheorem*{H1}{Temporal Functionality Heuristic}
\newtheorem*{H2}{Temporal Burstiness Heuristic}
\newtheorem*{H3}{One Event-Mention Per Discourse Heuristic}



In this section, we propose a set of temporal heuristics that are
useful to cluster relations at high precision. Our heuristics start
from the basic observation mentioned previously --- events can often
be uniquely determined by their arguments and time.  Additionally, we
find that it is not just the {\em publication time} of the news story
that matters, the {\em verb tenses} of the sentences are also
important. For example, the two sentences \mtt{``Armstrong was the
  chairman of Livestrong''} and \mtt{``Armstrong steps down from
  Livestrong''} have past and present tense respectively, which
suggests that the relation phrases are less likely to describe the
same event and are thus not semantically equivalent. To capture these
intuitions, we propose the {\em Temporal Functionality Heuristic}:

\begin{H1}
	\label{hypo_one}
        News articles published at the same time that mention the
        same entities and use the same tense tend to describe the
        same events.
% News articles published at the same time, mentioning the same
% entities, and using the same tense tend to describe the same events.
\end{H1}


Unfortunately, we find that not all the event candidates,
$(a_1,a_2,t)$, are equally good for clustering. For example, today's
news might include both \mtt{``Barack Obama heads to the White
  House''} and \mtt{``Barack Obama greets reporters at the White
  House''}. Although the two sentences are highly similar, sharing
$a_1 = $ ``Barack Obama'' and $a_2 = $ ``White House,'' and were
published at the same time, they describe different events.

From a probabilistic point of view, we can treat each sentence as
being generated by a particular hidden event which involves several
actors.  Clearly, some of these actors, like Obama, participate in
many more events than others, and in such cases we observe sentences
generated from a {\em mixture} of events.  Since two event mentions
from such a mixture are much less likely to denote the same event or
relation, we wish to distinguish them from the better (semantically
homogeneous) \eec s like the \mtt{(Armstrong, Livestrong)}
example. The question becomes ``How one can distinguish good entity
pairs from bad?''

Our method rests on the simple observation that an entity which
participates in many different events on one day is likely to have
participated in events in recent days. Therefore we can judge whether
an entity pair is good for clustering by looking at the {\em
  history of the frequencies} that the entity pair is mentioned in the
news streams, which is the {\em time series} of that entity pair. The
time series of the entity pair \mtt{(Barack Obama, the White House)}
tends to be high over time, while the time series of the entity pair
\mtt{(Armstrong, Livestrong)} is flat for a long time and suddenly
spikes upwards on a single day. This observation leads to:

\begin{H2}
	\label{hypo_two}
If an entity or an entity pair appears significantly more frequently
in one day's news than in recent history, the corresponding event
candidates are likely to be good for relation clustering.
\end{H2}

The temporal burstiness heuristic implies that a good
\eec\ $(a_1,a_2,t)$ tends to have a {\em spike} in the time series of
its entities $a_i$, or argument pair $(a_1,a_2)$, on day $t$.

However, even if we have selected a good \eec\ for clustering,
it is likely that it contains a few relation phrases that are related to
(but not synonymous with) the other relations included in the
\eec. For example, it's likely that the news story reporting
\mtt{``Armstrong steps down from Livestrong.''} might also mention
\mtt{``Armstrong is the founder of Livestrong.''} and so both ``steps
down from'' and ``is the founder of'' relation phrases would be part
of the same \bag.  Inspired by the idea of one sense per discourse
from ~\cite{gale1992one}, we propose:

\begin{H3}
	\label{hypo_three}
A news article tends not to state the same fact more than once.
\end{H3}


The one event-mention per discourse heuristic is proposed in order to gain
precision at the expense of recall --- the heuristic directs an algorithm
to choose, from a news story, the single ``best'' relation phrase
connecting a pair of two entities.  Of course, this doesn't answer the
question of deciding which phrase is ``best.''  In
Section~\ref{section_joint_cluster_model}, we describe how to learn a
probabilistic graphical model which does exactly this.


\section{Exploiting the Temporal Heuristics}
\label{section_model}

In this section we propose several models to capture the temporal
correspondence heuristics, and discuss their pros and cons.

\subsection{Baseline Model}
\label{section_basic_model}

An easy way to use an \bag\ is to simply predict that all $r_i$
%The baseline model to use the \bag\ is to simply predict that all $r_i$
in the \bag\ are event-mentions, and hence are semantically equivalent. That is, given \bag\ $(a_1,a_2,t,\{r_1\ldots r_m\})$, the output relation cluster is $\{r_1\ldots r_m\}$.

This baseline model captures the most of the temporal functionality heuristic, except for the tense requirement. Our empirical study
shows that it performs surprisingly well. This demonstrates that the quality of our input for the learning model is good: the \bag s are
promising resources for relation clustering.

Unfortunately, the baseline model cannot deal with the other
heuristics, a problem we will remedy in the following sections.

\subsection{Pairwise Model}
\label{section_news_spike}
The temporal functionality heuristic suggests we exploit the tenses
of the relations in an \bag; while the temporal burstiness heuristic suggests we exploit the time series of its arguments. A pairwise model can be designed to capture them: we compare pairs of relations in the
\bag, and predict whether each pair is synonymous or
non-synonymous.  Output clusters are then generated
according to some heuristic rules (\eg\ assuming transitivity among synonyms). The tenses of the relations and time series of the arguments are encoded as features, which we call {\em tense features} and {\em spike features} respectively. An example tense feature is whether one relation is past tense while the other relation is present tense; an example spike feature is the covariance of the time series.

The pairwise model can be considered similar to paraphrasing
techniques which examine two sentences and determine whether they are semantically equivalent~\cite{dolan2005automatically,SocherEtAl2011:PoolRAE}. In
section~\ref{empirical_study}, we evaluate the
effect of applying paraphrasing techniques for relation clustering.

\subsection{Joint Cluster Model}
\label{section_joint_cluster_model}
The pairwise model has several drawbacks: \textit{1)} it lacks the ability to handle constraints, such as the mutual exclusion constraint implied by the one-mention per discourse heuristic; \textit{2)} ad-hoc rules, rather than formal optimizations, are required to generate clusters containing more than two relations.

A common approach to overcome the drawbacks of the pairwise model and
to combine heuristics together is to introduce a joint cluster
model, in which heuristics are encoded as features and constraints. Data, instead of ad-hoc rules, determines the relevance of different insights, which can be learned as parameters. The advantage of the
joint model is analogous to that of cluster-based approaches for
coreference resolution (CR). In particular, a
joint model can better capture constraints on multiple variables and can yield higher quality results than pairwise CR
models~\cite{rahman2009supervised}.


We propose a undirected graphical model, \sys, which
jointly clusters relations. Constraints are captured by
factors connecting multiple random variables. We introduce random
variables, the factors, the objective function, the inference
algorithm, and the learning algorithm in the following sections.
Figure~\ref{graphmodel} shows an example model for \eec\
\mtt{(Armstrong, Livestrong, Oct 17)}.


\begin{figure}
\centering
\includegraphics[width=3.1in]{graphmodel.pdf}
\caption{an example model for \eec\ (Armstrong, Livestrong, Oct 17). $Y$ and $Z$ are binary random variables. $\Phi^Y$, $\Phi^Z$ and $\Phi^{\text{joint}}$ are factors. \mtt{be founder of} and \mtt{step down} come from article 1 while \mtt{give speech at}, \mtt{be chairman of} and \mtt{resign from} come from article 2.}
\label{graphmodel}
\end{figure}

\subsubsection{Random Variables}

For the \bag\ $(a_1,a_2,t,\{r_1,\ldots r_m\})$, we introduce one event
variable and $m$ relation variables, all boolean valued. The
event variable $Z^{(a_1,a_2,t)}$ indicates whether $(a_1,a_2,t)$ is a good event for clustering. It is designed in accordance with
the temporal burstiness heuristic: for the \eec\ \mtt{(Barack Obama, the White House, Oct 17)}, $Z$ should be assigned the value 0.

The relation variable $Y^r$ indicates whether relation $r$ describes
the \eec\ $(a_1,a_2,t)$ or not (\ie\ $r$ is an event-mention or not). The set of all event-mentions with $Y^r=1$ define a relation cluster. For example, the assignments $Y^{\textit{step\ down}}=Y^{\textit{resign from}}=1$ produce a relation cluster \mtt{\{step down, resign from\}}.

%The relation variable $Y^r$ indicates whether relation $r$ describes the \eec\ $(a_1,a_2,t)$ or not. All event-describing relations with $Y^r=1$ could compose a cluster. For example, the assignments $Y^{\textit{step\ down}}=Y^{\textit{resign}}=1$ produce a relation cluster \mtt{\{step down, resign\}}.

\subsubsection{Factors and the Joint Distribution}

In this section, we introduce a conditional probability model defining
a joint distribution over all of the event and relation variables. The
joint distribution is a function over {\it factors}. Our model
contains {\em event factors}, {\em relation factors} and {\em joint factors}.

The event factor $\Phi^Z$ is a log-linear function with spike
features, used to distinguish good events. A relation factor
$\Phi^Y$ is also a log-linear function. It can be defined for
individual relation variables (\eg\ $\Phi^Y_1$ in Figure \ref{graphmodel})
with features such as whether a relation phrase comes from a clausal
complement\footnote{Relation phrases in clausal complement are less useful for clustering because they often do not describe a fact. For example, in the sentence \mtt{He heard Romney had won the election}, the
extraction (Romney, had won, the election) is not a fact at all.}.
A relation factor can also be defined for a pair of relation variables (\eg\ $\Phi^Y_2$ in Figure~\ref{graphmodel}) with features capturing
the pairwise evidence for clustering, such as if two relation phrases
have the same tense.


The joint factors $\Phi^{\text{joint}}$ are defined to apply constraints
implied by the temporal heuristics. They play two roles in our model:
\textit{1)} to satisfy the temporal burstiness heuristic, when the value of
the event variable is false, the \eec\ is not appropriate for clustering,
and so all relation variables should also be false; and \textit{2)} to
satisfy the one-mention per discourse heuristic, at most one relation
variable from a single article could be true.

We define the joint distribution over these variables and factors as
follows. Let $\mathbf{Y}=(Y^{r_1}\ldots Y^{r_m})$ be the vector of relation
variables; let $\mathbf{x}$ be the features. The joint distribution is:

\begin{eqnarray*}
\lefteqn{p(Z = z, \mathbf{Y} = \mathbf{y}|\mathbf{x}; \Theta) \stackrel{\text{\tiny def}}{=} \frac{1}{Z_{x}}\Phi^{Z}(z,\mathbf{x}) }\\
    && \times \prod_d \Phi^{\text{joint}}(z,\mathbf{y}_d,\mathbf{x})
        \prod_{i,j} \Phi^Y (y_i,y_j,\mathbf{x})
\end{eqnarray*}
\noindent
where $\mathbf{y}_d$ indicates the subset of relation variables from a
particular article $d$, and the parameter vector $\Theta$ is the weight vector of the features in $\Phi^Z$ and $\Phi^Y$, which are log-linear
functions; \ie,
\[
\Phi^{Y}(y_i,y_j,\mathbf{x}) \stackrel{\text{\tiny def}}{=}
\exp \left( \sum_j \theta_{j} \phi_j(y_i,y_j,\mathbf{x}) \right)
\]
where $\phi_j$ is the $j$th feature function.

The joint factors $\Phi^{\text{joint}}$ are used to apply the temporal
burstiness heuristic and the one event-mention per discourse
heuristic.  $\Phi^{\text{joint}}$ is zero when the \eec\ is not good
for clustering, but some $y^r=1$; or when there is more than one $r$ in a single article such that $y^r=1$.
Formally, it is calculated as:
\[
\Phi^{\text{joint}}(z,\mathbf{y}_d,\mathbf{x})
\stackrel{\text{\tiny def}}{=}
\begin{cases}
\footnotesize
0  & {\rm if} ~ z=0 \land \exists  y^r = 1  \\
0  & {\rm if} ~ \sum_{y^r\in \mathbf{y}_d} y^r > 1  \\
1  & {\rm otherwise} \\
\end{cases}
\]










\subsubsection{Maximum a Posteriori Inference} The goal of inference is to
find the predictions $z,\mathbf{y}$ %dce which yield the greatest
probability, \ie, $z^*,\mathbf{y}^* = \arg\max_{z,\mathbf{y}} p(Z=z,
\mathbf{Y} = \mathbf{y}|\mathbf{x}; \Theta)$. This can be viewed as a MAP
inference problem.




In general, inference in a graphical model is challenging.
Fortunately, the joint factors in our model are linear, and the event and relation factors are log-linear; we can cast MAP inference as an integer linear programming (ILP) problem, and then compute an approximation in polynomial time by means of linear  programming using randomized rounding, as proposed in~\cite{Yannakakis92}.


We build one ILP problem for every \eec. The variables of the ILP
are $Z$ and $\mathbf{Y}$, which only take values of 0 or 1.
The objective function is the sum of logs of the event and relation
factors $\Phi^Z$ and $\Phi^Y$. The temporal burstiness heuristic  of $\Phi^{\text{joint}}$ is encoded as a linear inequality constraint
$z\geq y_i$; the one-mention per discourse heuristic of $\Phi^{\text{joint}}$ is encoded as the constraint $\sum_{y_i\in \mathbf{y}_d} y_i \leq 1$.

\def\gold{^{\mathsf{gold}}}

\subsubsection{Learning}

Our training data consists of $N=500$ labeled \bag\ in the form of
$\{(R_i,R\gold_i)\mid_{i=1}^N\}$. Each $R$ is the set of all relations
in the \bag\ while $R\gold$ is a manually created subset of $R$
containing relations describing the \eec. $R\gold$ could be empty if
the \eec\ is not good for clustering. For our model, the gold
assignment ${y^r}\gold=1$ if $r\in R\gold$; the gold assignment
$z\gold=1$ if $R\gold$ is not empty.

Given $\{(R_i,R\gold_i)\mid_{i=1}^N\}$, learning over similar models is commonly done via maximum likelihood estimation as follows:

\[
L(\Theta) = \log \prod_i p(Z_i=z\gold_i,\mathbf{Y}_i=\mathbf{y}\gold_i\mid\mathbf{x}_i,\Theta)
\]

For features in relation factors, the partial derivative for the $i$th model is: %dce
\[
\Phi_j(\mathbf{y}\gold_i,\mathbf{x}_i)-E_{p(z_i,\mathbf{y}_i\mid ,\mathbf{x}_i,\Theta)}\Phi_j(\mathbf{y}_i,\mathbf{x}_i)
\]
where $\Phi_j(\mathbf{y}_i, \mathbf{x}_i)=\sum
\phi_j(X,Y,\mathbf{x})$, the sum of values for the $j$th
feature in the $i$th model; and values of $X,Y$ come from the assignment
$\mathbf{y}_i$. For features in event factors, the partial derivative is derived similarly as
\[
\phi_j(z\gold_i,\mathbf{x}_i)-E_{p(z_i,\mathbf{y}_i\mid ,\mathbf{x}_i,\Theta)}\phi_j(z_i,\mathbf{x}_i)
\]

It is unclear how to efficiently compute the expectations in the above formula,
%$E_{p(z_i,\mathbf{y}_i\mid  ,\mathbf{x}_i,\Theta)}\phi_j(z_i,\mathbf{x}_i)$ and $E_{p(z_i,\mathbf{y}_i\mid   ,\mathbf{x_i},\Theta)}\Phi_j(\mathbf{y}_i,\mathbf{x}_i)$:
a brute force approach requires enumerating all assignments of $\mathbf{y}_i$, which is exponentially large with the number of relations. Instead, we
opt to use a more tractable perceptron
learning approach~\cite{collins02,hoffmann2011knowledge}. Instead of computing the expectations, we simply compute $\phi_j(z^*_i,\mathbf{x}_i)$ and
$\Phi_j(\mathbf{y}^*_i,\mathbf{x}_i)$, where $z^*_i,\mathbf{y}^*_i$ is
the assignment with the highest probability, generated by the MAP
inference algorithm using the current weight vector. The weight
updates are the following:
\begin{align}
&\Phi_j(\mathbf{y}\gold_i,\mathbf{x}_i) - \Phi_j(\mathbf{y}^*_i,\mathbf{x}_i)\label{update_relation_variable}\\
&\phi_j(z\gold_i,\mathbf{x}_i) - \phi_j(z^*_i,\mathbf{x}_i)
\end{align}

The updates can be intuitively explained as penalties on errors. In
sum, our learning algorithm consists of iterating the following two steps: (1) infer the most probable assignment given the current weights; (2) update the weights by comparing inferred assignments and the truth
assignment.

%Note that the focus of this paper is to obtain high precision results, we make a simple modification: we penalize the weight more on false positives than on false negatives. For example, when seeing a false positive $1=z^*_i>z\gold_i=0$, we multiply the update value by a rate $\delta>1$; otherwise the rate is $1$. Similar procedure can be applied to Equation \ref{update_relation_variable}.











\section{Empirical Study}
\label{empirical_study}
We first introduce the experimental setup for our empirical study, and then we attempt to answer two questions in sections~\ref{section_eval_parallel} and~\ref{section_methods_of_distributional} respectively: First, does the \sys\ algorithm effectively exploit the proposed heuristics and outperform other approaches which also use news streams? Secondly, do the proposed temporal heuristics cluster relations with greater precision than the distributional hypothesis?


\subsection{Experimental Setup}
\label{section_data_generation}

Since we were unable to find any suitable time-stamped, parallel, news
corpus, we collected data using the following procedure:

\bi

\item Collect RSS news seeds, which contain the title, time-stamp, and
abstract of the news items.

\item Use these titles to query the Bing news search engine API and collect
additional time-stamped news articles.

\item Strip HTML tags from the news articles using
Boilerpipe~\cite{kohlschutter2010boilerplate}; keep only the title and
first paragraph of each article.

\item Extract shallow relation tuples using the OpenIE
system~\cite{fader11}.  \ei

We performed these steps every day from January 1 to February 22, 2013.  In
total, we collected 546,713 news articles, for which 2.6 million
extractions had 529 thousand unique relations. These led to 79,427 \bag s.

We used several types of features for clustering:
\textit{1)} spike features obtained from time series;
\textit{2)} tense features, such as whether two relation phrases are both in the present tense;
\textit{3)} cause-effect features, such as whether two relation phrases often appear successively in the news articles;
\textit{4)} text features, such as whether sentences are similar;
\textit{5)} syntactic features, such as whether a relation phrase appears in a clausal complement; and
\textit{6)} semantic features, such as whether a relation phrase contains negative words.

\subsection{Comparison with Methods using Parallel News Corpora}
\label{section_eval_parallel}
We evaluated \sys\ against other methods that also use time-stamped news.
These include the models mentioned in section \ref{section_temporal} and state-of-the-art paraphrasing techniques.


Human annotators created gold relation clusters for 500 \bag s; note that
some \bag s yield no gold cluster, since at least two synonymous phrases.
Precision and recall were computed by comparing an algorithm's output
clusters to the gold cluster of each \eec. We consider paraphrases with
minor lexical diversity, \eg\ \mtt{(go to, go into)}, to be of lessor
interest. Since counting these trivial paraphrases tends to exaggerate the
performance of a system, we also report precision and recall on {\em
diverse clusters} \ie, those whose relation phrases all have different head
verbs. Figure~\ref{f:pr_example} illustrates these metrics with an example;
note under our diverse metrics, all phrases matching \mtt{go\ *} count as
one when computing both precision and recall.  We conduct 5-fold cross
validation on our labeled dataset to get precision and recall numbers when
the system requires training.

{\begin{figure}[t]
\def\textexample#1{\it\scriptsize #1}
\renewcommand\arraystretch{1.3}
\centering
\small
\begin{tabular}{|>{\centering\arraybackslash}m{0.1\textwidth}|
>{\centering\arraybackslash}m{0.34\textwidth}|}\hline
%\begin{tabular}{c|c|c}\hline
\small
 $\textbf{output}$ &  \{\bmtt{go\ into}, \bmtt{go\ to},  \mtt{speak},  \mtt{return}, \bmtt{head\ to}\}\\\hline
 $\textbf{gold}$ &\{\bmtt{go\ into}, \bmtt{go\ to}, \mtt{approach}, \bmtt{head\ to}\}\\\hline
  $\textbf{gold}_{\textrm{div}}$ &\{\bmtt{go\ *}, \mtt{approach}, \bmtt{head\ to}\}\\\hline
  $\textbf{P/R}$ &
$\textrm{precision}=3/5$\
$\textrm{recall}=3/4$\\\hline
 $\textbf{P/R}_{\textrm{div}}$ &
$\textrm{precision}_{\textrm{div}}=2/4$\
$\textrm{recall}_{\textrm{div}}=2/3$\\\hline
\end{tabular}
\caption{\label{f:pr_example} an example pair of the output cluster and the gold cluster, and the corresponding precision recall numbers; we bold the phrases appearing in both the output and gold clusters. }
\end{figure}
}

We compare \sys\ to the following approaches:

{\bf Baseline:} the model discussed in
Section~\ref{section_basic_model}. This system does not need any training,
and generates outputs with perfect recall.

{\bf Pairwise:} the pairwise model discussed in
Section~\ref{section_news_spike} and using the same set of features as used
by \sys. To generate output clusters, transitivity is assumed inside the
\bag. For example, when the pairwise model predicts that $(r_1,r_2)$ and
$(r_1,r_3)$ are both paraphrases, the resulting cluster is
$\{r_1,r_2,r_3\}$.


{\bf Paraphrase:} Socher~\etal~\shortcite{SocherEtAl2011:PoolRAE} achieved
the best results on the Dolan~\etal~\shortcite{dolan2004unsupervised}
dataset, and released their code and models. We used their off-the-shelf
predictor to replace the classifier in our Pairwise model. Given sentential
paraphrases, aligning relation phrases is natural, because OpenIE has
already identified the relation phrases.


\begin{table}[bt]
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
 \hline
 \multirow{2}{*}{ System} & \multicolumn{2}{c|}{ P/R }  & \multicolumn{2}{c|}{ $\textrm{P/R}_{\textrm{div}}$} \\
\cline{ 2-5}
         & ~prec~ & ~rec~  & ~prec~ & ~rec~ \\\hline
Baseline & 0.67 & 1.00 & 0.53 & 1.00\\
Pairwise & 0.90 & 0.60 & 0.84 & 0.35\\
Paraphrase & 0.81 & 0.37 & 0.68 & 0.31 \\\hline
%w/oSpike & 90.2 & 62.5 & 83.7 & 28.2\\\hline
\sys\ & \bf{0.92} & 0.60 & \bf{0.90} & 0.38\\
\hline
\end{tabular}
\end{center}
\caption{Comparison with methods using parallel news corpora} \label{t:compare_to_parallel}
\end{table}

Table~\ref{t:compare_to_parallel} shows precision and recall numbers.  It
is interesting that the basic model already obtains $0.67$ precision
overall and $0.53$ in the diverse condition. This demonstrates that the
\bag s generated from the news streams are a promising resource for
clustering.  Paraphrase performs better, but not as well as Pairwise or
\sys, especially in the diverse cases. This is probably due to the fact
that Paraphrase is purely based on text metrics and does not consider any
temporal attributes. Taking into account the features used by \sys,
Pairwise significantly improves the precision, which demonstrates the power
of our temporal correspondence heuristics.  Our joint cluster model, \sys,
which considers both temporal features and constraints, gets the best
performance in both conditions.

We conducted ablation testing to evaluate how spike features and tense
features, which are particularly relevant to the temporal aspects of news
streams, can improve performance. Figure~\ref{ablation} compares the
precision/recall curves for three systems in the diverse condition: (1)
\sys; (2) w/oSpike: turning off all spike features; and (3) w/oTense:
turning off all features about tense.  There are some dips in the curves
because they are drawn after sorting the predictions by the value of the
corresponding ILP objective functions, which do not perfectly reflect
prediction accuracy. However, it is clear that \sys\ produces greater
precision over all ranges of recall.


\begin{figure}
\centering
\includegraphics[width=3.1in]{ablation.pdf}
\caption{Precision recall curves on diverse cases for NewsSpike, w/oSpike and w/oTense}
\label{ablation}
\end{figure}




\subsection{Comparison with Methods using the Distributional Hypothesis}
\label{section_methods_of_distributional} We evaluated our model against
methods based on the distributional hypothesis. We ran \sys\ over all \bag
s except for the development set and compared to the following systems:

{\bf Resolver: } Resolver~\cite{yates2009unsupervised} uses a set of
extraction tuples in the form of $(a_1,r,a_2)$ as the input and creates a
set of relation clusters as the output\footnote{Resolver also produces
argument clusters, but this paper only evaluates relation
clustering}.\comment{Resolver applies a hybrid similarity metrics by
combining Extracted Shared Property similarity (each relation is
represented by a vector of argument pairs) and string similarity (each
relation is represented by a vector of its own words).} We evaluated
Resolver's performance with an input of the 2.6 million extractions
described in section \ref{section_data_generation}, using Resolver's
default parameters.


{\bf ResolverNYT: } Since Resolver is supposed to perform better when given
more accurate statistics from a larger corpus, we tried giving it more
data. Specifically, we ran ReVerb on 1.8 million NY Times articles
published between 1987 and 2007 obtain 60 million
extractions~\cite{sandhaus08}.  We ran Resolver on the union of this and
our standard test set, but report performance only on clusters whose
relations were seen in our news stream.

{\bf ResolverNytTop: } Resolver is designed to achieve good performance on
its top results. We thus ranked the ResolverNYT outputs by their scores and
report the precision of the top 100 clusters.

{\bf Cosine: } Cosine similarity is a basic metric for the distributional
hypothesis. This system employs the same setup as Resolver in order to
generate relation clusters, except that Resolver's similarity metric is
replaced with the cosine. Each relation is represented by a vector of
argument pairs. The similarity threshold to merge two clusters was 0.5.

{\bf CosineNYT: } As for ResolverNYT, we ran CosineNYT with an extra
60 million extractions and reported the performance on relations seen in
our news stream.




%{\bf CosineNYT: } Consider the size of extractions in our parallel corpus is comparably smaller, in this baseline we investigate the potential of distribution similarity by applying cosine similarity on a large set of extractions. We use the New York Times~\cite{sandhaus08} as the text set, which contains over 1.8 million news articles published between January 1987 and June 2007. We run Reverb over the text and get about 60 million extractions. We add them to the extractions of our parallel corpus. To make the numbers comparable, we subset the output relation clusters by only considering the relations seen in the parallel corpus.

We measured the precision of each system by manually labeling all output if
100 or fewer clusters were generated (\eg ResolverNytTop), otherwise 100
randomly chosen clusters were sampled. Annotators first determined the
meaning of every output cluster and then created a gold cluster by choosing
the correct relations\footnote{The gold cluster could be empty if the
output cluster was nonsensical}. Unlike many papers that simply report
recall on the most frequent relations, we evaluated the total number of
returned relations in the output clusters. As in
Section~\ref{section_eval_parallel}, we also report numbers for the case
of lexically diverse relation phrases.


%in the same way as Resolver~\cite{yates2009unsupervised}, a relation $r$ in the output cluster is labeled as correct if the majority of relations in that cluster are synonyms of $r$. The precision is the number of relations in the output clusters which are correct dividing the total number of relations in output clusters. Measure overall recall requires labeling all relations, which is impossible. The recall of Resolver is measured on 200 most frequent relations. We are interested in clustering all relations, more than most frequent ones. So we simply compare the total number of relations in output clusters for each system.

%We notice that clusters with relation phrases sharing the same head verb (\eg {\tt (convert to, convert into)} are very likely to be correct clusters. But simple clusters like this are less interesting, and counting them could exaggerate the precision of a system. In this paper, we include another metric by reporting numbers after ignoring all simple cases. We only take into account relations in an output cluster that do not share the majority head verb of that cluster. For example, in a cluster {\tt (convert to, convert into, turn into)}, only {\tt turn into} counts. To our best knowledge, we are the first paper to introduce this challenging metric.



\begin{table}[bt]
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|c|}
 \hline
\multirow{2}{*}{ System} & \multicolumn{2}{c|}{ all }  & \multicolumn{2}{c|}{ diverse } \\
\cline{ 2-5}
         & ~prec~ & \#rels & ~prec~ & \#rels \\\hline
{ Resolver} & 0.78 & 129 & 0.65 & 57 \\
{ ResolverNyt} & 0.64 & 1461 & 0.52 & 841 \\
{ ResolverNytTop} & 0.83 & 207 & 0.72 & 79 \\
{Cosine} & 0.65 & 17 & 0.33 & 9 \\
{CosineNyt} & 0.56 & 73 & 0.46 & 59 \\\hline
\sys\ & \bf{0.93} & 21472 & \bf{0.87} & 5368 \\
\hline
\end{tabular}
\end{center}
\caption{Comparison with methods using the distributional hypothesis} \label{t:compare_to_distributional}
\end{table}

As can be seen in Table~\ref{t:compare_to_distributional}, \sys\ outperformed methods based on the distributional hypothesis. The performance
of the Cosine and CosineNyt was very low, suggesting that simple similarity
metrics are insufficient for handling the relation clustering problem, even
when large-scale input is involved. Resolver and ResolverNyt employ an
advanced similarity measurement and achieve better results. However, it is
surprising that Resolver results in a greater precision than
ResolverNyt. It is possible that argument pairs from news streams spanning
20 years sometimes provide incorrect evidence for clustering. For example,
there were extractions like \mtt{(the Rangers, be third in, the NHL)} and
\mtt{(the Rangers, be fourth in, the NHL)} from news in 2007 and 2003
respectively. Using these phrases, ResolverNyt produced the incorrect
cluster \mtt{\{be third in, be fourth in\}}. \sys\ achieves greater
precision than even the best results from ResolverNytTop, because \sys\
successfully captures the temporal heuristics, and does not confuse
synonyms with antonyms, or causes with effects.  \sys\ also returned on
order of magnitude greater number of relations than other
methods.\comment{This is because given two relations in news streams,
conclusions can only be drawn from distributional similarities when given
large amounts of shared extractions; this is not common even given the
whole NY Times corpus as input. When distributional similarities are
confronted with insufficient statistical evidences, \sys\ can still predict
many relations correctly by exploiting the strength of temporal
information.}

The heuristics and models in this paper are proposed for high precision. To compare with the distributional hypothesis, we purposely forced \sys\ not to rely on any distribution similarity. But \sys's graphical model has the flexibility to incorporate any similarity metrics as features. Such a hybrid model has great potential to increase recall, which is one goal for future work.


%Within a single EEC, rule out a lot of bad hypotheses, disagree in tense, draw strong conclusion from small amount of data, dis sim needs a lot of data to draw any conclusion.




%Comparing the numbers of { ResolverNyt} and {ResolverNytTop}, we find that the top results of { ResolverNyt} are good but the precision quickly diminishes on the less confident outputs.

%It demonstrates the drawback of the distributional hypothesis with regards to the less frequent relations, in such cases the system is not capable of obtaining sufficient statistical evidences. \sys\ captures the temporal hypotheses and achieves greater precision than ResolverNytTop, it has also returned more clusters than {ResolverNyt}.


%\subsection{Discussion}


%{\bf Resolver} using extractions from news articles during one month does not heavily suffer from such errors because there are few input extractions sharing arguments but having opposite meanings.



%Table~\ref{t:compare_to_distributional} shows that \sys\ significantly outperforms methods of distributional hypothesis.  {\bf Cosine} and {\bf CosineNyt} performs very bad. It suggests that simple distributional similarity metric is far from solving relation clustering problem. {\bf Resolver} and {\bf ResolverNyt} achieve better results. But it is surprising that {\bf ResolverNyt} get higher precision than {\bf Resolver}, although {\bf ResolverNyt} takes 60 millions extra extractions as the input. A possible reason is that the quality of the extra input is much lower. For example there are extractions like \texttt{(the Rangers, be third in, the NHL)} and \texttt{(the Rangers, be forth in, the NHL)} from news in 2007 and 2003 respectively. Extractions like this yield incorrect output cluster \texttt{be third in / be forth in}. {\bf Resolver} using extractions from news articles during one month does not heavily suffer from such errors because there are few input extractions sharing arguments but bearing different or even opposite meanings. Comparing the numbers of {\bf ResolverNyt} and {\bf ResolverNytTop}, we find that the top results of {\bf ResolverNyt} are much better but the precision drops quickly on less confident outputs. It shows that distributional hypothesis is weak on less frequent relations, on such cases not enough evidence is obtained.

%Numbers above show that distributional hypothesis is less robust than one may think of. The error analysis also suggest that temporal information could be very useful. \sys\ captures \temporal\ achieve  higher precision than {\bf ResolverNytTop} and more returns than {\bf ResolverNyt}.

%\subsection{Error Analysis}




% It achieves higher precision than {\bf ResolverNytTop} and more returns than {\bf ResolverNyt}.
%
%It is surprising that {\bf Resolver} get better precision than {\bf ResolverNyt}, which take 60 millions extractions as the extra input. Lower quality input is the reason: for example there are extractions like \texttt{(the Rangers, be third in, the NHL)} and \texttt{(the Rangers, be forth in, the NHL)} from news in 2007 and 2003 respectively. Such extractions yield incorrect output \texttt{be worse / be best}. {\bf Resolver} using extractions from parallel news does not heavily suffer from such errors because the input quality is higher. The above example also shows the importance of using temporal evidences.
%
%
%there are too many noise in large scale extractions so the problem of confusing antonyms and synonyms becomes much sever.
%
%The top results of {\bf Resolver} are good its performance drops quickly


%{\bf Cosine: } In this baseline we use the same framework as Resolver but replace the similarity metric with cosine similarity. The input is the extractions from the parallel corpus.
%
%{\bf CosineNYT: } Similar to ResolverNYT, we add 60 million extractions from New York Time corpus to the input.




%\subsection{Experimental Setup}
%
%%{\bf Input: } with streams of parallel news articles, we identify the set of $(a_1,r,a_2)$ tuples from sentences of these articles by out-of-shelf open information extraction system Reverb~\cite{Fader11}.
%
%{\bf Input: } As we discussed, the News Spike algorithm models the potential event $(a_1,a_2,t)$ and whether the relation phrases $r$ in the form of $(r,a_1,a_2,t)$ is describing the event.
%%Given the news articles with time-stamps, instances of our model are generated by taking in all argument pairs whenever the Open Information Extraction system tells us there is a tuple $(a_1,r,a_2)$; and
%We are more interested in precision in this paper so we setup day-by-day time intervals. (\ie $t$ will be a day like Dec14. We only cluster relation phrases when they are from the news on the same day).
%
%%Every sentence in one specific bag share the same mentions of argument pairs.
%{\bf Output: } For each instance, we output a relation cluster by putting relation phrases of those true relation variable. We ignore those cluster containing zero or only one relation phrase.
%
%%For each instance, identify whether there is an event and whether each relation phrase is a good representation. It could happen that all mentions are identified as false simply because there does not exist any significant event for the argument pair of that instance.
%
%%{\bf Supervision: } To collection training data and to evaluate the performance, we randomly sample some bags and manually annotate all tuples in these bags.
%
%{\bf Evaluation Metrics: } a phrase $r$ in $(r,a_1,a_2,t)$ is manually labeled as positive if $r$ describe the event of $(a_1,a_2,t)$, otherwise it is labeled as negative. Let $R$ denotes the set of inferred positive relations by the model whereas $R\gold$ denotes the set of positive relation labeled by human beings. Then precision and recall can be computed in standard way.
%
%%Yates~\shortcite{yates2009unsupervised} design the recall metrics for only top clusters. In this work, we care all output produced by our system. So we will report the total number of output relation clusters of different systems. We also report the number of cases when they are antonyms. We also report pairwise precision and recall.
%
%Since annotators cannot create the complete relation clusters, one cannot directly compare recall numbers between \temporal\ systems and distributional similarity systems. We report the number of relations in clusters as the evaluation metric.
%
%%as the degree that how far one system reaches the limitation of its design. For example, Yates~\shortcite{yates2009unsupervised} design the recall metrics for only top clusters.
%
%\subsection{Baseline systems}
%
%%{\bf distribution similarity: } we compute the similarity between all pairs of relation phrases by two measures: cosine similarity and ESP proposed by Yates. We look at the 1000 pairs with highest scores and manually mark whether they are synonymous.
%
%{\bf Resolver: } Resolver is the state-of-the-art relation clustering system based on computing distribution similarity. It proposed the ESP probability model, and the idea of co-clustering arguments and relation phrases. Resolver is a complicated system with many processing and parameters. We implement a basic version by following core algorithm in~\cite{yates2009unsupervised} \footnote{The code to compute ESP similarity is provided by the author of Resolver}. We apply it on our datasets, which contains 1.2 millions tuples.
%
%{\bf Resolver+WS: } We want to fully exploit the potential of distribution similarity by applying Resolver on a web scale dataset and compare with our work. We use the New York Times~\cite{sandhaus08} as the text set, which contains over
%1.8 million news articles published between Jan. 1987 and Jun. 2007. We run Open IE on all the text and get about 60 million extractions. To make the numbers comparable, we apply Resolver to only cluster the relations appearing in our parallel dataset.
%
%{\bf LDA:} ~\cite{yao2011structured} propose a LDA-style approach to identify instances of some specific relations. We adopt their idea here as a baseline system: we treat the co-occurrence of relation phrases and argument objects as a word-document matrix; then an LDA was running on this matrix to cluster relation phrases into its hidden topics.
%
%{\bf DIRT:} Following the work of DIRT, we randomly pick 100 relation phrases in our bags, submit it to the DIRT system and manually label the top
%
%{\bf w/o News Spike: } remove the news spike features of \sys\ .
%
%{\bf w/o joint inference} Remove the joint inference component of \sys\ . So we simply conduct a pairwise classification.

%{\bf w/o joint inference w/ post-processing} In order to show the advantage of joint inference, we conduct a series of post-processing on the output of w/o joint inference system. We firstly ... secondly ... thirdly...

%\subsection{Performance of News Spike}
%We test several variations of News Spike model. In this group of experiments, we randomly sample 300 argument pairs, build the News Spike model, and manually assign values to all clustering variables. So we can evaluate both precision and recall here.
%
%\begin{table*}[bt]
%\footnotesize
%\begin{center}
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
% \hline
%\multirow{2}{*}{ System} & \multicolumn{2}{c|}{ ind. }  & \multicolumn{2}{c|}{ w/o simple } & \multicolumn{2}{c|}{ pairs }  & \multicolumn{2}{c|}{ pairs w/o simple }\\
%\cline{ 2-9}
%         & accuracy & \#clusters & accuracy & \#clusters
%         & accuracy & \#pairs & accuracy & \#pairs  \\\hline
%csm & 0.50 & 1 & 0.46 & 1 & 0.50 & 1 & 0.46 & 1\\
%csm Nyt & 0.50 & 1 & 0.46 & 1 & 0.50 & 1 & 0.46 & 1\\
%Resolver & 0.50 & 1 & 0.46 & 1 & 0.50 & 1 & 0.46 & 1\\
%Resolver Nyt & 0.50 & 1 & 0.46 & 1 & 0.50 & 1 & 0.46 & 1\\
%Resolver Nyt Top & 0.50 & 1 & 0.46 & 1 & 0.50 & 1 & 0.46 & 1\\
%\sys\ & 0.50 & 1 & 0.46 & 1 & 0.50 & 1 & 0.46 & 1\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Performance of News Spike} \label{t:twtrandom}
%\end{table*}
%
%It can be seen clearly that News Spike algorithm significantly improve the precision of the relation clustering algorithm, which is the major goal of this work. Moreover, the news spike features and joint inference boost the precision a lot. It is interesting to see the naive baseline achieve 50\% precision, which means
%
%
%
%\subsection{Performance of Distribution Similarity}
%We evaluate the precision of returned relation clusters. Note that sometimes the cluster is composed of relation phrases with the same head verbs, \eg {\tt stay in} and {\tt stay at}. Recall the goal of relation clustering is to remove the syntactic variations, so it is more interesting to see how system performs on phrases with different head verbs.
%
%We manually annotate all output clusters of Resolver; we random sample 100 output clusters of ResolverWS and annotate them. We also annotate the top 100 clusters of ResolverWS.
%
%\begin{table}[bt]
%\footnotesize
%\begin{center}
%\begin{tabular}{|c|c|c|c|c|}
% \hline
%\multirow{2}{*}{ System} & \multicolumn{2}{c|}{ all}  & \multicolumn{2}{c|}{ different head verbs } \\
%\cline{ 2-5}
%         & Precision & \#clusters & Precision & \#clusters \\
%\hline
%Resolver   &   92    &   153    &  85 &   61 \\
%ResolverWS & 63 & 4287 & 51 & 3287\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Performance of distribution similarity} \label{t:twtrandom}
%\end{table}
%
%Resolver using our parallel news corpus is getting pretty good precision (while the number of returned cluster is very small). But it is surprising to see that Resolver on the whole New York Times is getting poor performance. The reason is that the distribution of two relation phrases are similar to each other but their shared argument pairs are not informative at all. For example, relation phrases {\tt range from} and {\tt start at} are sharing following argument pairs:
%
%\begin{table}[bt]
%\begin{center}
%\begin{tiny}
%\begin{tabular}{c}
%\hline
%(Admission prices, \$ 4)(Their main obstacle, tiny ink and chalk sketches) \\
%(Double rooms, \$ 36) (the moods, idyllic to bumptious) (Prices, \$ 190,000)   \\
%(New York City public school teachers, \$ 31,910) (Prices, \$ 195) (prices, \$ 195) \\
%(Admission prices, \$ 10) (Prices, \$ 1,950) (Double rooms, \$ 210)  (Prices, \$ 19)\\
%(Prices, \$ 190) (The work, inflected images)  (The Early Rounds prices, \$ 2,495)\\
% (Double rooms, \$ 295) (Double rooms, \$ 350) (Double rooms, \$ 270)(the third tier, \$ 6.50) \\
%\hline
%\end{tabular}
%\end{tiny}
%\end{center}
%\caption{\sys\ shared argument pairs for relation phrase {\tt range from} and {\tt start at}} \label{t:matching}
%\end{table}
%
%Resolver on our parallel dataset achieves high accuracy because the argument pairs are more likely to point to certain news event than normal unlabeled dataset. (Recall we are querying news titles on search engine to collect the dataset. So argument pairs about event may occur many times.) In this case, argument pairs are providing stronger evidence. So when two relation phrase share many argument pairs, it is likely that they are real synonyms.
%
%
%\subsection{Performance of LDA-style algorithms}
%Have not been done yet.


%\subsection{Analysis}
%We need some numbers to validate our claims:
%\bi
%\item How many new relation clusters can we get everyday? the accuracy?
%\item Do we still suffer from confusing antonyms with synonyms?
%\item can we achieve good accuracy on low frequent relation phrases?
%\ei


%%
%%
%%\section{Experimental Setup}
%%
%%We follow the approach of Riedel~\etal~\shortcite{riedel-ecml10} for
%%generating weak supervision data, computing features, and evaluating
%%aggregate extraction.
%%
%%We also introduce new metrics for measuring sentential extraction
%%performance, both relation-independent and relation-specific.
%%
%%\subsection{Data Generation}
%%
%%We used the same data sets as Riedel~\etal~\shortcite{riedel-ecml10} for
%%weak supervision.  The data was first tagged with the Stanford NER
%%system~\cite{finkel-acl05} and then entity mentions were found by
%%collecting each continuous phrase where words were tagged identically (\ie,
%%as a person, location, or organization).  Finally, these phrases were
%%matched to the names of Freebase entities.
%%
%%Given the set of matches, define $\Sigma$ to be set of NY Times sentences
%%with two matched phrases, $E$ to be the set of Freebase entities which were
%%mentioned in one or more sentences, $\Delta$ to be the set of Freebase
%%facts whose arguments, $e_1$ and $e_2$ were mentioned in a sentence in
%%$\Sigma$, and $R$ to be set of relations names used in the facts of
%%$\Delta$.  These sets define the weak supervision data.
%%
%%\subsection{Features and Initialization}
%%
%%We use the set of sentence-level features described by
%%Riedel~\etal~\shortcite{riedel-ecml10}, which were originally developed by
%%Mintz~\etal~\shortcite{mintz-acl09}.  These include indicators for various
%%lexical, part of speech, named entity, and dependency tree path properties
%%of entity mentions in specific sentences, as computed with the Malt
%%dependency parser~\cite{nivre04} and OpenNLP POS
%%tagger\footnote{http://opennlp.sourceforge.net/}.  However, unlike the previous work, we did not make
%%use of any features that explicitly aggregate these properties across
%%multiple mention instances.
%%
%%The \sys\ algorithm has a single parameter $T$, the number of training iterations, that must
%%be specified manually. We used $T=50$ iterations, which performed best in development
%%experiments.
%%
%%
%%\subsection{Evaluation Metrics}
%%\label{s:metrics}
%%
%%%We use evaluation metrics designed to test accuracy at the aggregate level, by evaluating the set of facts that are extracted, and at the sentence level, by evaluating the quality of individual extractions.
%%
%%Evaluation is challenging, since only a small percentage (approximately 3\%)
%%of sentences match facts in Freebase,
%%and the number of matches is highly unbalanced across relations, as we will see in more detail later. We
%%use the following metrics.
%%
%%\paragraph{Aggregate Extraction} Let $\Delta^e$ be the set of extracted
%%relations for any of the systems; we compute aggregate precision and recall
%%by comparing $\Delta^e$ with $\Delta$.
%%  This metric is easily computed but underestimates
%%extraction accuracy because Freebase is incomplete and some true relations
%%in $\Delta^e$ will be marked wrong.
%%
%%\paragraph{Sentential Extraction}  Let $S^e$ be the sentences where some
%%system extracted a relation and $S^F$ be the sentences that match the
%%arguments of a fact in $\Delta$.  We manually compute sentential extraction
%%accuracy by sampling a set of 1000 sentences from $S^e \cup S^F$ and
%%manually labeling the correct extraction decision, either a relation $r \in
%%R$ or $none$.   We then report precision and recall for each system on this
%%set of sampled sentences.   These results provide a good approximation to
%%the true precision but can overestimate the actual recall, since we did not
%%manually check the much larger set of sentences where no approach predicted
%%extractions.
%%
%%
%%\subsection{Precision / Recall Curves}
%%To compute precision / recall curves for the
%%tasks, we ranked the \sys\ extractions as follows.   For sentence-level evaluations, we
%%ordered according to the extraction factor score $\Phi^{\text{extract}}(z_i,x_i)$. For aggregate
%%comparisons, we set the score for an extraction $Y^r=true$ to be the max of the extraction
%%factor scores for the sentences where $r$ was extracted.
%%
%%
%%
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\section{Experiments}
%%\label{s:exp}
%%
%%To evaluate our algorithm, we first compare it to an existing approach for using multi-instance learning with weak supervision~\cite{riedel-ecml10}, using the same data and features.   We report both aggregate extraction and sentential extraction results.
%%We then investigate relation-specific performance of our system. Finally, we report running time comparisons.
%%%We then describe a simple method for improving performance with better data generation and compare running times of the various approaches.
%%
%%%The \sys\ extractions were done, as described in Section~\ref{s:inf}, by
%%%independently extracting the highest scoring relation for each sentence,
%%%and using all of the extracted facts for aggregate level prediction.
%%
%%\begin{figure}[bt]
%%\hspace*{-10pt}
%%\includegraphics[width=3.3in]{Relation.pdf}
%%%\vspace*{1.5in}
%%\vspace{-24pt}
%% \caption{Aggregate extraction precision / recall curves for
%%   Riedel~\etal~\shortcite{riedel-ecml10}, a reimplementation of that
%%   approach ({\bf SoloR}), and our algorithm ({\bf MultiR}).}
%%   \label{f:agg}
%%\end{figure}
%%
%%\subsection{Aggregate Extraction}
%%
%%Figure~\ref{f:agg} shows approximate precision / recall curves for three
%%systems computed with aggregate metrics (Section~\ref{s:metrics}) that test
%%how closely the extractions match the facts in Freebase.  The systems
%%include the original results reported by
%%Riedel~\etal~\shortcite{riedel-ecml10} as well as our new model (\sys).  We
%%also compare with {\bf SoloR}, a reimplementation of their algorithm, which we
%%built in Factorie~\cite{mccallum2009factorie}, and will use later to evaluate
%%sentential extraction.
%%
%%\sys\ achieves competitive or higher precision over all ranges of recall,
%%with the exception of the very low recall range of approximately 0-1\%.  It also
%%significantly extends the highest recall achieved, from 20\% to 25\%, with
%%little loss in precision.  To investigate the low precision in the 0-1\% recall
%%range, we manually checked the ten highest confidence extractions
%%produced by \sys\ that were marked wrong. We found that all
%%ten were true facts that were simply missing from Freebase.  A manual evaluation, as we perform next
%%for sentential extraction, would remove this dip.
%%
%%\subsection{Sentential Extraction}
%%
%%
%%%\begin{table*}[bt]
%%%\centering
%%%\small{
%%%\begin{tabular}{|l|r|r||l|l||l|l||l|l|}
%%%\hline
%%%\multicolumn{1}{|c|}{Relation} & \multicolumn{2}{c||}{ Overlaps} & \multicolumn{2}{c||}{ Baseline1 } & \multicolumn{2}{c||}{ Baseline2 } & \multicolumn{2}{c|}{\sys } \\
%%%  & \#pairs & \#snts & $\tilde{P}_m$ & $\tilde{R}_m$ & $\tilde{P}_m$ & $\tilde{R}_m$ & $\tilde{P}_m$ & $\tilde{R}_m$ \\
%%%\hline
%%%/business/person/company                      &  1 &   2 & .971& .371& .941 & .360 & .74  & .427 \\
%%%/people/person/place\_lived                   & 63 &  97 & .833& .083& .833 & .083 & .87  & .083 \\
%%%/location/location/contains                   & 96 & 708 & 1   & .255& 1    & .51  & .8   & .59  \\
%%%/business/company/founders                    &  0 &   0 & .889& .174& .667 & .13  & .667 & .13  \\
%%%/people/person/nationality                    &  3 &   4 & 1   & .146& 1    & .22  & .59  & .22  \\
%%%/location/neighborhood/neighborhood\_of       &  0 &   0 & 1   & .111& 1    & .111 & .469 & .111 \\
%%%/people/person/children                       &  0 &   0 & 1   & .083& 1    & .125 & .786 & .125 \\
%%%/people/deceased\_person/place\_of\_death     & 19 &  36 & 1   & .2  & 1    & .267 & .905 & .267 \\
%%%/people/person/place\_of\_birth               & 59 &  82 & 1   & .083& 1    & .083 & .286 & .25  \\
%%%/business/company/advisors                    &  0 &   0 & N/A & 0   & N/A  & 0    & N/A  & 0    \\
%%%/location/country/administrative\_divisions   & 60 & 412 & N/A & 0   & N/A  & 0    & N/A  & 0    \\
%%% other relations with lots of overlaps: /location/country/capital (487), /location/us_state/capital (39) ...
%%%  Overlapping combinations (%pairs, %snts):
%%%1       1       /people/person/place_of_birth,/people/deceased_person/place_of_burial,/people/person/place_lived
%%%42      145     /location/location/contains,/location/country/administrative_divisions
%%%14      221     /location/location/contains,/location/country/capital
%%%2       11      /location/province/capital,/location/location/contains
%%%9       39      /location/location/contains,/location/us_state/capital
%%%3       3       /people/person/place_lived,/people/person/place_of_birth,/people/deceased_person/place_of_death
%%%1       2       /business/person/company,/people/person/nationality
%%%17      266     /location/location/contains,/location/country/capital,/location/country/administrative_divisions
%%%9       23      /location/location/contains,/location/us_county/county_seat
%%%1       1       /people/person/place_lived,/people/person/nationality
%%%24      28      /people/person/place_of_birth,/people/person/place_lived
%%%24      40      /people/person/place_lived,/people/person/place_of_birth
%%%10      24      /people/person/place_lived,/people/deceased_person/place_of_death
%%%1       1       /location/location/contains,/base/locations/countries/states_provinces_within,/location/country/administrative_divisions
%%%2       2       /location/location/contains,/location/br_state/capital
%%%1       1       /people/person/place_of_birth,/people/person/nationality
%%%6       9       /people/person/place_of_birth,/people/deceased_person/place_of_death
%%
%%% Overlapping relations (%pairs, %snts):
%%%96      708     /location/location/contains
%%%59      82      /people/person/place_of_birth
%%%63      97      /people/person/place_lived
%%%9       23      /location/us_county/county_seat
%%%1       1       /people/deceased_person/place_of_burial
%%%3       4       /people/person/nationality
%%%1       1       /base/locations/countries/states_provinces_within
%%%2       11      /location/province/capital
%%%1       2       /business/person/company
%%%31      487     /location/country/capital
%%%2       2       /location/br_state/capital
%%%60      412     /location/country/administrative_divisions
%%%19      36      /people/deceased_person/place_of_death
%%%9       39      /location/us_state/capital
%%
%%
%%
%%%\hline
%%%\end{tabular}
%%%}
%%%\caption{Number of overlapping matches to Freebase, as well as estimated precision and recall for \sys\ and two baselines. }
%%%\label{t:sent2}
%%%\vspace{-8pt}
%%%\end{table*}
%%
%%%Baseline1 overall: pr .977, re .224
%%%Baseline2 overall: pr .981, re .380
%%
%%
%%
%%
%%\begin{figure}[bt]
%%\hspace*{-10pt}
%%\includegraphics[width=3.3in]{Sent.pdf}
%%%\vspace*{1.5in}
%%\vspace{-24pt}
%% \caption{Sentential extraction precision / recall curves for \sys\ and
%%   {\bf SoloR}.}
%%   \vspace{-5pt}
%%   \label{f:sent}
%%\end{figure}
%%
%%Although their model includes variables to model sentential extraction,
%%Riedel~\etal~\shortcite{riedel-ecml10} did not report sentence level
%%performance.  To generate the precision / recall curve we used the joint
%%model assignment score for each of the sentences that contributed to
%%the aggregate extraction decision.
%%
%%
%%Figure~\ref{f:agg} shows approximate precision / recall curves for
%%\sys\ and {\bf SoloR} computed against manually generated sentence labels, as defined in Section~\ref{s:metrics}.   \sys\ achieves significantly higher recall with a consistently high level of precision.    At the highest recall point, \sys\ reaches 72.4\% precision and 51.9\% recall, for
%%an F1 score of 60.5\%.
%%
%%%\begin{itemize}
%%%\item say how we sampled up to 100 for each relation
%%%\item if we did not reweight (then bias towards performance of dominant relation /location/location/contains, then precision 98.2\% at recall 43.4\%.
%%%\item how we extended the curve
%%%\end{itemize}
%%
%%\subsection{Relation-Specific Performance}
%%
%%Since the data contains an unbalanced number of instances of each relation, we
%%also report precision and recall for each of the ten most frequent relations.
%%Let $S^{M}_r$ be the sentences where \sys\ extracted an instance of relation
%%$r \in R$, and let $S^F_r$ be the sentences that match the arguments of a
%%fact about relation $r$ in $\Delta$. For each $r$, we sample 100 sentences
%%from both $S^{M}_r$ and $S^F_r$ and manually check accuracy. To estimate
%%precision $\tilde{P}_r$ we compute the ratio of true relation mentions in
%%$S^{M}_r$, and to estimate recall $\tilde{R}_r$ we take the ratio of true relation mentions
%%in $S^F_r$ which are returned by our system.
%%
%%\begin{table*}[bt]
%%\begin{small}
%%\begin{center}
%%\begin{tabular}{|c|r|c||l|l|}
%%\hline
%%\multirow{2}{*}{Relation} & \multicolumn{2}{c||}{Freebase Matches} & \multicolumn{2}{c|}{ \sys} \\
%%         & ~\#sents~~ & \% true & $\tilde{P}$ & $\tilde{R}$ \\
%%\hline
%%/business/person/company                     & 302  & 89.0  & 100.0  & 25.8 \\
%%/people/person/place\_lived                  & 450  & 60.0   & ~~80.0  & ~~6.7 \\
%%/location/location/contains                  & 2793 & 51.0  & 100.0  & 56.0  \\
%%/business/company/founders                  & 95   & 48.4 & ~~71.4 & 10.9  \\
%%/people/person/nationality                   & 723  & 41.0  & ~~85.7  & 15.0  \\
%%/location/neighborhood/neighborhood\_of      & 68   & 39.7 & 100.0 & 11.1 \\
%%/people/person/children                       & 30   & 80.0   & 100.0 & ~~8.3 \\
%%/people/deceased\_person/place\_of\_death     & 68   & 22.1 & 100.0 & 20.0 \\
%%/people/person/place\_of\_birth              & 162  & 12.0  & 100.0 & 33.0  \\
%%/location/country/administrative\_divisions  & 424  & ~~0.2 & N/A  & ~~0.0    \\
%%\hline
%%\end{tabular}
%%\end{center}
%%\end{small}
%%\caption{Estimated precision and recall by relation, as well as the number of matched sentences (\#sents) and accuracy (\%~true) of matches between sentences and facts in Freebase. }
%%\label{t:sent}
%%\end{table*}
%%
%%Table~\ref{t:sent} presents this approximate precision and recall for \sys\ on each of the relations,
%%along with statistics we computed to measure the quality  of the weak supervision.
%%Precision is high for the majority of relations but recall is consistently lower.
%%%However, the matches to Freebase, too, have low recall (25.7\%\footnote{On a sample of 2000 annotated
%%%sentences, we found that match recall is about 25.7\% and precision around 41.3\%.}).
%%We also see that the Freebase matches are highly skewed in quantity
%%and can be low quality for some relations,
%%with very few of them actually corresponding to true extractions.
%%The approach generally performs best on the relations with a sufficiently large number of true matches,
%%in many cases even achieving precision that outperforms the accuracy of the heuristic matches, at reasonable
%%recall levels.
%%
%%\subsection{Overlapping Relations}
%%
%%Table~\ref{t:sent} also highlights some of the effects of learning with overlapping relations.  For example, in the data, almost all of the matches for the administrative\_divisions relation overlap with the contains relation, because they both model relationships for a pair of locations.  Since, in general, sentences are much more likely to describe a contains relation, this overlap leads to a situation were almost none of the administrate\_division matches are true ones, and we cannot accurately learn an extractor. However, we can still learn to accurately extract the contains relation, despite the distracting matches.   Similarly, the place\_of\_birth and place\_of\_death relations tend to overlap, since it is often the case that people are born and die in the same city.   In both cases, the precision outperforms the labeling accuracy and the recall is relatively high.  %However,  the birth place is much harder to learn to predict overall, given the  low match accuracy.
%%
%%To measure the impact of modeling overlapping relations, we also evaluated a simple, restricted baseline.  Instead of labeling each entity pair with the set of all true Freebase facts, we created a dataset where each true relation was used to create a different training example. Training \sys\ on this data simulates effects of conflicting supervision that can come from not modeling overlaps.   On average across relations, precision increases 12 points but recall drops 26 points, for an overall reduction in F1 score from 60.5\% to 40.3\%.
%%%the recall for this baseline drops approximately 20 points to 22.4\% and the precision drops almost 5 points to 72.9\%.
%%% for baseline 2 F1 score reaches .56.
%%
%%
%%%\begin{itemize}
%%%\item challenge, vast number of NAs: in fact, 96.9\% of entity pairs do
%%%not have any matches to Freebase
%%%\item quality of the matches: on a sample of about 2000 sentences, we found that
%%%across relations matching recall only around 25.7\%, matching precision around 41.3\%
%%%\item evaluating relation-specific quality based on a manually annotated sample of the test set unpractical, due to small number of mentions with relations
%%%\item instead we approximate precision and recall by sampling ....
%%
%%%\item unsuprisingly, the amount and quality of matches varies between relations, and so does the quality of our predictor. Table X gives an overview
%%
%%%\item observation 1: the extractors often reach high precision, sometimes at the cost of low recall. w
%%
%%%\item observation 2: some relation highly correlated with target, but not the same: administrative\_divisions. Here, the matches are particularly poor, since they usually are sentences containing a location contains, but not administrative relationship.
%%
%%%\item we also note skew in relations
%%
%%%\item somewhere: how we labeled (facts that are entailed), e.g. nationality, place\_lived
%%
%%%\end{itemize}
%%
%%
%%
%%%\begin{figure}[bt]
%%%\includegraphics[width=3.1in]{Relabeled.pdf}
%%%%\vspace*{1.5in}
%%%\vspace{-8pt}
%%% \caption{Precision / recall curves for \sys's aggregate extraction on
%%%   relabeled and original data. }
%%%   \label{f:relabel}
%%%   \vspace{-8pt}
%%%\end{figure}
%%
%%%\subsection{Improving Performance}
%%
%%%We also report experiments with modifications to the original
%%%weak-supervision data matching algorithm and the set of features used in
%%%the model.  When matching entities from Freebase to the text, we allow
%%%matches to the list of pseudonyms, instead of only using the primary name.
%%%This change significantly increases the number of matching facts, from 4700
%%%to 5500, approximately, in the training set.
%%
%%%Figure~\ref{f:relabel} shows the aggregate extraction precision / recall
%%%curves for \sys\ with the old and new data sets.  The 17\% increase in
%%%training data creates a large improvement in performance, demonstrating the
%%%advantage of careful matching strategies for weak supervision.  It also
%%%suggests that weak-supervision approaches may achieve significantly
%%%improved performance, as the size and quality of the reference database
%%%grows.
%%
%%
%%\subsection{Running Time}
%%
%%One final advantage of our model is the modest running time.   Our implementation of the Riedel~\etal~\shortcite{riedel-ecml10} approach required approximately 6 hours to train on NY Times 05-06 and 4 hours to test on the NY Times 07, each without preprocessing.   Although they do sampling for inference, the global aggregation variables require reasoning about an exponentially large (in the number of sentences) sample space.
%%
%%In contrast, our approach required approximately one minute to train and less than one second to test, on the same data.    This advantage comes from the decomposition that is possible with the deterministic OR aggregation variables.   For test, we simply consider each sentence in isolation and during training our approximation to the weighted assignment problem is linear in the number of sentences.
%%
%%\subsection{Discussion}
%%The sentential extraction results demonstrates the advantages of learning a model that is primarily driven by sentence-level features.
%%Although previous approaches have used more sophisticated features for aggregating the evidence from individual sentences, we demonstrate that aggregating strong sentence-level evidence with a simple deterministic OR that models overlapping relations is more effective, and also enables  training of a sentence extractor that  runs with no  aggregate information.
%%
%%While the Riedel~\etal\ approach does include a model of which sentences express relations, it makes significant use of aggregate features that are primarily designed to do entity-level relation predictions and has a less detailed model of extractions at the individual sentence level.   Perhaps surprisingly, our model is able to do better at both the sentential and aggregate levels.
%%



\section{Related Work}

The vast majority of paraphrasing work fall into two categories:
approaches based on the distributional hypothesis or on correspondences
between parallel
corpora~\cite{androutsopoulos2009survey,madnani2010generating}.

{\bf Using Distribution Similarity:} Lin and
Pantel's~\shortcite{lin2001discovery} DIRT employs mutual information
statistics to compute the similarity between relations represented in
dependency paths.  Resolver~\cite{yates2009unsupervised} introduces a
new similarity metric called the Extracted Shared Property (ESP) and
uses a probabilistic model to merge ESP with surface string
similarity.

Identifying the semantic equivalence of relation phrases is also
called {\it relation discovery} or {\it unsupervised semantic
  parsing}.  Often techniques don't compute the similarity explicitly
but rely implicitly on the distributional hypothesis. Poon and
Domingos~\shortcite{poon2009unsupervised} USP clusters relations
represented with fragments of dependency trees by repeatedly merging
relations having similar
context. Yao~\etal~\shortcite{yao2011structured,yaounsupervised}
introduces generative models for relation discovery using LDA-style
algorithm over a relation-feature matrix.
Chen~\etal~\shortcite{chen2011domain} focuses on domain-dependent
relation discovery, extending a generative model with meta-constraints
from lexical, syntactic and discourse regularities.

Our work solves a major problem with these approaches, avoiding errors
such as confusing synonyms with antonyms and causes with
effects. Furthermore, \sys\ doesn't require massive statistical
evidence as do most approaches based on the distributional hypothesis.



{\bf Using Parallel Corpora: } Comparable and parallel corpora,
including news streams and multiple translations of the same story,
have been used to generate paraphrases, both
sentential~\cite{barzilay2003learning,dolan2004unsupervised,shinyama2003paraphrase}
and
phrasal~\cite{barzilay2001extracting,shen2006adding,pang2003syntax}. Typical
methods first gather relevant articles and then pair sentences that
are potential paraphrases.  Given a training set of paraphrases,
models are learned and applied to unlabeled
pairs~\cite{dolan2005automatically,SocherEtAl2011:PoolRAE}. Phrasal
paraphrases are often obtained by running an alignment algorithm over
the paraphrased sentence pairs.

While prior work uses the temporal aspects of news streams as a coarse
filter, it largely relies on text metrics, such as context similarity and
edit distance, to make predictions and alignments. These metrics are
usually insufficient to produce high precision results; moreover they tend
to produce paraphrases that are simple lexical variants (\eg\ {\it \{go to,
go into\}.}). In contrast, \sys\ generates relation clusters with both high
precision and high diversity.

{\bf Others:}
Textual entailment~\cite{dagan2009recognizing}, which finds a phrase
implying another phrase, is closely related to the relation clustering task. Berant~\etal~\cite{berant2011global} notes the flaws in
distributional similarity and proposes local entailment
classifiers, which are able to combine many features.
Lin~\etal~\cite{lin2012no} also uses temporal information to detect
the semantics of entities.





\section{Conclusion}

Relation clustering is crucial to many natural language processing
applications, including relation extraction, question answering,
summarization, and machine translation. Unfortunately, previous
approaches based on distribution similarity and parallel corpora,
often produce low precision clusters. This paper introduces three
Temporal Correspondence Heuristics that characterize semantically
equivalent phrases in news streams. We present a novel algorithm,
\sys, based on a probabilistic graphical model encoding these
heuristics, which harvests high-quality relation clusters.


% The joint model of \sys\ is also flexible in its ability to include
% new features and constraints.

Experiments show \sys's improvement relative to several other methods,
especially at producing lexically diverse clusters.  Ablation tests
confirm that our temporal features are crucial to \sys's precision. In
order to spur future research, we are releasing an annotated corpus of
time-stamped news articles and our harvested relation clusters.

%%  Additionally, this work introduces a new evaluation
%% metric which considers only diversified relation clusters which are
%% more useful but are challenging to generate. Empirical study shows
%% that \sys\ outperforms other methods in precision by considerable
%% margins, especially on the diversified relation clusters. This
%% demonstrates the power of the proposed temporal heuristics and the
%% effectiveness of our joint model. 




%For example, textual entailment~\cite{dagan2009recognizing}, which finds a phrase inferring another phrase, is very related to relation clustering task. It is natural to conjecture that certain temporal hypotheses are useful to textual entailment; relation clusters are used in different tasks in various ways, it is interesting to develop a general interface to improve the performances of different downstream NLP tasks.






%We argue that weak supervision is promising method for scaling
%information extraction to the level where it can handle the myriad,
%different relations on the Web.  By using the contents of a database
%to heuristically label a training corpus, we may be able to
%automatically learn a nearly unbounded number of relational
%extractors.  Since the processs of matching database tuples to
%sentences is inherently heuristic, researchers have proposed
%multi-instance learning algorithms as a means for coping with the
%resulting noisy data. Unfortunately, previous approaches assume that
%all relations are {\em disjoint} --- for example they cannot extract
%the pair {\tt Founded(Jobs, Apple)} and {\tt CEO-of(Jobs, Apple)},
%because two relations are not allowed to have the same arguments.

%This paper presents a novel approach for multi-instance learning with
%overlapping relations that combines a sentence-level extraction model
%with a simple, corpus-level component for aggregating the individual
%facts.  We apply our model to learn extractors for NY Times text using
%weak supervision from Freebase. Experiments show improvements
%for both sentential and aggregate (corpus level) extraction, and demonstrate
%that the approach is computationally efficient.

%Our early progress suggests many interesting directions.
%By joining two or more Freebase tables, we can generate many more
%matches and learn more relations. We also wish to refine our model in
%order to improve precision. For example, we would like to add type
%reasoning about entities and selectional preference constraints for
%relations.  Finally, we are also interested in applying the overall learning
%approaches to other tasks that could be modeled with weak supervision,
%such as coreference and named entity classification.
