\section{Literature Review}

Manual annotation for NLP training data is well-known for its tedium
and large amount of data. To generate a comprehensive annotated
training set requires much human effort. Annotators are also prone to
make mistakes during the long and tedious annotating process.
Researchers are trying to address these problems by two means: 1)
develop visualization tools to improve annotation efficiency as well
as reduce the error rate in annotation; 2) adopt crowdsourcing to
enable collaborative annotation that accelerates the process of
annotation.

Most related in scope is \cite{yan2012collaborative} which provides a
collaborative tool to assist annotators in tagging of complex Chinese
and multilingual linguistic data. It visualizes a tree model that
represents the complex relations across different linguistic elements
to reduce the learning curve. Besides it proposes a web-based
collaborative annotation approach to meet the large amount of data.
Their tool only focuses on a specific area that is complex
multilingual linguistic data, whereas our work is trying to address
how to generate a visualization model for general data sets.

Crowdsourcing \cite{howe2006rise} is a popular and fast growing
research area. There have been a lot of studies on understanging what
it is and what it can do. For instance, \cite{quinn2009taxonomy}
categorizes crowdsourcing into seven genres: Mechanized Labor, Game
with a Purpose (GWAP), Widom of Crowds, Crowdsourcing, Dual-Purpose
Work, Grand Serarch, Human-based Genetic Algorithms and Knowledge
Collection from Volunteer Contributors. Other works, such as
\cite{abekawa2010community} and \cite{irvine2010using}, develops a
specific tool and verifies the feasibility and benefit of
crowdsourcing. It is generally convinced that crowdsourcing is of
great beneficial if the tasks are easy to conduct by the workers and
the tasks are independent.

Because of the high labor requirements in typical NLP training tasks,
there also have been some work considing using crowdsoursing in many
NLP tasks. For example, Grady et. al. generated a data set on document
relevance to search queries for information
retrieval~\cite{Grady2010CDR18666961866723}; Negri et. al. built a
cross-lingual textual corpora~\cite{Negri2011DCC21454322145510};
Finin et. al. collected simple named entity annotations using Amazon MTurk
and Crowd-Flower~\cite{Finin2010ANE18666961866709}. Also there are
some researchers observed the hardness of collecting high quality data
and did some studies on improving that, such
as~\cite{Hsueh2009DQC15641311564137}( how annotations should be
selected to maximize quality), and \cite{lease2011quality} (quality
control in crowdsoursing by machine learning).

Different from previous studies, we seek to improve crowdsoursing
annotating quality by greatly lower the usability barrier through the
proposed visualized toolkit rather than trying to cleaning up the data
generated by the crowdsoursing process. 
