\section{Introduction}
Statistical machine learning approaches has made great successes in research disciplines such as parsing~\cite{klein2003accurate}, information extractions~\cite{banko2007open}, question answering~\cite{kwok2001scaling}. Yet off-the-shelf models learned from training data of one particular domain (usually newswire) would often underperform at present tasks, whose data could come from other domains such as social media~\cite{zhang2013adaptive}. To fully exploit the power of statistical approaches, it is useful to quickly collect plentiful in-domain training data in a short period of time.

With the advent of crowd-sourcing platforms (e.g. Amazon Mechanical Turk\footnote{http://www.mturk.com} and Odesk \footnote{http://www.odesk.com}), it becomes realistic to quickly hire a large group of crowd-sourced workers with a fair amount of cost. For example, there are over 500,000 workers in AMT and in average, they are more educated than the United States population. But when the NLP data is annotated by unevenly-trained non-expert workers, errors are rampant~\cite{snow2008cheap}. We believe they happen for at least three reasons: (1) sophisticated linguistic practices, for example, the guidelines for penn treebank are more than 300 pages~\cite{bies1995bracketing}. It is impossible for non-experts to catch all the details; (2) comprehensive operators caused by sophisticated rules, for example, to generate a parsed tree, annotators must identify the POS of each tree node from a set of hundreds tags; (3) many NLP problems are structured prediction problem, when the labels depend on each other, each decision requires deep lookahead and backtracking.

Therefore, it is not surprising that the outputs of the crowd-sourced workers are far from oracles. During annotations, they are having limited NLP knowledge, are blind about the dependencies of the data, but having excessive amount of options to choose. To alleviate the problem, we believe a good visualization toolkit for crowd-sourced workers could dramatically increase their productivity and accuracy. Unfortunately, existing toolkits either provide limited visual aid, or introduce comprehensive operators to realize sophisticated linguistic practices and restrict their pool of workers. Instead of anticipating the workers to be NLP experts, we aim to allow ordinary people to annotate the training corpus. For this, the toolkit is designed with several principles in mind: (1) simplified operators: comprehensive operators for crowd workers often mean longer education time and lower accuracy. We thus only allow users to {\em click} and {\em draw} over the data. The tradeoff between expressiveness and quantity is worthwhile because we can hire crowd-sourced workers to generate more training data with less errors. (2) interactive interface for deep lookahead and backtracking, workers should efficiently read the dependency of the data in some interactive way; (3) convenient trial and error, for structured prediction, labels are related to each other while human beings have to annotate them in some serialized manner. So it is inevitable that workers would frequently fix their errors; (4) generalization, many toolkits only specialize on one problem because they must provide comprehensive and specialized operators. As oppose to them, our toolkit with simplified operators is consequently easier to cover a broader range of NLP tasks.

In this paper, we present \sys, a visualization toolkit for crowd-sourced NLP annotation. The key observation is that many NLP problems turn out to be clustering and parsing. On one hand, NLP system is often asked to identify the relationship between objects, which could be words (\eg\ paraphrasing), mentions (\eg\ co-reference), sentences (\eg\ summarization), documents (\eg\ sentiment analysis), etc. All these tasks could be treated as some practice of clustering. On the other hand, many NLP applications rely on the results of parsing. For example, relation extraction~\cite{hoffmann2011knowledge} approaches use the dependency features generated from the parsed trees; some syntax-based translation method~\cite{chiang2010learning} translate between trees of different languages. \sys\ allows crowd-sourced workers to generate the clustering graphs and the parsing trees. They can annotate the training data with simplified operators to avoid tedious education; they can backtrack their labels in an interactive interface to better read the data; they can easily edit their previous actions to quickly fix the errors. User studies show our toolkit is very friendly to NLP non-experts, and allow them to produce high quality labels for several sophisticated problems.  The source code and demo of \sys\ are available at \url{http://}.

The rest of this paper is organized as follows. In Section 2 we demonstrate the overview of our toolkit. Section 3 and 4 presents the implementations for clustering and parsing annotation respectively. In Section 5 we introduce the evaluation metrics, and present the experimental results of our toolkit with respect to several baselines. In section 6 we discuss the related work of our toolkit. Finally we conclude our work and present the discussion.








