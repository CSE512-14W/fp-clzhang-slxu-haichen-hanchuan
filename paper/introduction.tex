\section{Introduction}
Statistical machine learning approaches has made great successes in research disciplines such as parsing~\cite{klein2003accurate}, information extractions~\cite{banko2007open}, question answering~\cite{kwok2001scaling}. Yet off-the-shelf models learned from training data of one particular domain (usually newswire) would often underperform tasks at present, whose data come from other domains such as social media~\cite{zhang2013adaptive}. To fully exploit the power of statistical approaches, it is useful to quickly collect in-domain training data in a short period of time.

With the advent of crowd-sourcing platforms (e.g. Amazon Mechanical Turk\footnote{http://www.mturk.com} and Odesk \footnote{http://www.odesk.com}), it becomes realistic to quickly hire a large group of crowd-sourced workers with a fair amount of money. For example, there are over 500,000 workers in AMT and in average, they are more educated than the United States population. But when the NLP annotations are performed by unevenly-trained non-expert workers, errors are rampant~\cite{snow2008cheap}. We believe that NLP problems are particularly hard for non-experts, for at least three reasons: (1) sophisticated rules, for example, the guidelines for penn treebank are more than 300 pages~\cite{bies1995bracketing}. It is impossible for non-experts to catch these details; (2) comprehensive operators caused by sophisticated rules, for example, to generate a parsed tree, annotators must identify the POS of each tree node from a set of hundreds tags; (3) many NLP problems are structured prediction problem, when the labels depend on each other, each decision requires deep lookahead and backtracking. Therefore,
during annotations, workers often have limited knowledge of the problem, are blind about the dependencies of the data, but have excessive amount of options to perform. It is not surprising that their outputs are far from oracles.

We believe that it is useful to visualize NLP annotations so crowd-sourced workers could easily handle the task. Traditional toolkits, such as Brat~\cite{brat2014}, are often designed for NLP researchers or programmers. They provide a set of comprehensive operators so experts could realize sophisticated linguistic rules. Rather than restricting the workers to NLP experts, we aim to allow ordinary people to perform the NLP annotations. For this, the toolkit is designed with several principles in mind: (1) simplified operators: comprehensive operators for crowd workers often mean longer education time and more errors. We thus only allow users to {\em click} and {\em draw} over the data. The tradeoff between expressiveness and quantity is worthwhile because we can hire crowd-sourced workers to provide more data with less errors. (2) interactive interface for deep lookahead and backtracking, workers should efficiently read the dependency of the data in some interactive way; (3) convenient trial and error, for structured prediction, labels are related to each other while human beings have to annotate them in some serialized manner. So it is inevitable that workers would frequently fix their errors; (4) generalization, existing annotation tools mostly focus on sentential-level annotations and allow users to identify all nuances, they are thus great tools for NLP experts. As opposed to them, our toolkit with simplified operators is consequently easier to cover a broader range of NLP tasks.

In this paper, we present \sys, a visualization toolkit for crowd-sourced NLP annotation. They key observation is that many NLP problems turn out to be clustering and parsing. NLP system is often asked to identify the relationship between objects. The objects could be words (\eg\ paraphrasing), mentions (\eg\ co-reference), sentences (\eg\ summarization), documents (\eg\ sentiment analysis), etc. All these tasks could be treated as some sort of clustering. On the other hand, many NLP applications rely on the results of parsing. For example, relation extraction~\cite{hoffmann2011knowledge} approaches use features generated from parsed tree; some syntax-based translation method~\cite{chiang2010learning} translate between trees of different languages. \sys\ allows crowd-sourced workers to generate the clustering graphs and parsing trees. They can finish the jobs with simplified operators so tedious education is not necessary; they can backtrack their labels within an interactive interface; they can easily edit their previous actions to fix the errors. The source code and demo of \sys\ are available at \url{http://}.

The rest of this paper is organized as follows. In Section 2 we demonstrate the overview of our toolkit. Section 3 and 4 presents the implementations for clustering and parsing annotation respectively. In Section 5 we introduce the evaluation metrics, and present the experimental results of our toolkit with respect to several baselines. In section 6 we discuss the related work of our toolkit. Finally we conclude our work and present the discussion. 





 


