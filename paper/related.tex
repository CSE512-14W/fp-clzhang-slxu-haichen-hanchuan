\section{Related Work}

Many NLP tasks require large amount of high quality training data.
Manual annotation for such training data is well-known for its tedium.
To generate a comprehensive annotated training set requires much human
effort. Annotators are also prone to make mistakes during the long and
tedious annotating process.  Researchers are trying to address these
problems by two means: 1) building specialized annotating tools to
ease the annotating process in the hope of improving efficiency as
well as reducing the error rates; 2) adopting crowdsourcing to scale
up annotating.

\textbf{Specialized annotating tools}. Facing one of the biggest common problems, many NLP researchers have developed a number of tools
for annotating training corpora along the history of NLP research. At
  first, before the blossom of the web, tools are generally built as
  local programs such as the WordFreak linguistic annotation
  tool~\cite{Morton2003WOT} and the  UAM CorpusTool for
  text and image annotation~\cite{ODonnell2008DUC}.
  These tools are very restricted because they cannot scale. Web-based
  annotation tools are developed later in order to scale up the
  annotating process, such as~\cite{Stuhrenberg2007WAA}.
  However these tools typically only use very basic HTML based
  techniques to provide very limited visual aids for the annotating
  process. Most related in scope is~\cite{yan2012collaborative} which
  provides a collaborative tool to assist annotators in tagging of
  complex Chinese and multilingual linguistic data. It visualizes a
  tree model that represents the complex relations across different
  linguistic elements to reduce the learning curve. Besides it
  proposes a web-based collaborative annotation approach to meet the
  large amount of data.  Their tool only focuses on a specific area
  that is complex multilingual linguistic data, whereas our work is
  trying to address how to generate a visualization model for general
  data sets.


\textbf{Crowdsoursing in NLP}. Crowdsourcing \cite{howe2006rise} is a
popular and fast growing research area. There have been a lot of
studies on understanging what it is and what it can do. For instance,
\cite{quinn2009taxonomy} categorizes crowdsourcing into seven genres:
Mechanized Labor, Game with a Purpose (GWAP), Widom of Crowds,
Crowdsourcing, Dual-Purpose Work, Grand Serarch, Human-based Genetic
Algorithms and Knowledge Collection from Volunteer Contributors. Other
works, such as \cite{abekawa2010community} and \cite{irvine2010using},
develops a specific tool and verifies the feasibility and benefit of
crowdsourcing. It is generally convinced that crowdsourcing is of
great beneficial if the tasks are easy to conduct by the workers and
the tasks are independent.

Because of the high labor requirements in typical NLP training tasks,
there also have been some work considering using crowdsoursing in many
NLP tasks. For example, Grady \etal\ generated a data set on document
relevance to search queries for information
retrieval~\cite{Grady2010CDR18666961866723}; Negri \etal\ built a
cross-lingual textual corpora~\cite{Negri2011DCC21454322145510};
Finin \etal\ collected simple named entity annotations using Amazon MTurk
and Crowd-Flower~\cite{Finin2010ANE18666961866709}. Also there are
some researchers observed the hardness of collecting high quality data
and did some studies on improving that, such
as~\cite{Hsueh2009DQC15641311564137}( how annotations should be
selected to maximize quality), and \cite{lease2011quality} (quality
control in crowdsoursing by machine learning).

Different from previous studies, we seek to improve crowdsoursing
annotating quality by greatly lower the usability barrier through the
proposed visualized toolkit rather than trying to cleaning up the data
generated by the crowdsoursing process.
